[
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Defining Data Science - 4 + 1 Overview",
    "section": "",
    "text": "For a detailed review of these sources, see the Appendix.↩︎"
  },
  {
    "objectID": "design.html",
    "href": "design.html",
    "title": "Design",
    "section": "",
    "text": "It is important to note that all these areas are evolving and have overlapping content area, but Design in particular can be difficult to neatly fold into well-defined set of easily quantifiable skills."
  },
  {
    "objectID": "design-sds.html",
    "href": "design-sds.html",
    "title": "2  Design SDS",
    "section": "",
    "text": "Overview of the Curriculum and Skills that the School of Data Science and UVA believes to be included in the domain area of Design."
  },
  {
    "objectID": "design-lab.html",
    "href": "design-lab.html",
    "title": "3  Design Lab",
    "section": "",
    "text": "Overview of the Design lab for the current semester for Foundations of Data Science Course at UVA."
  },
  {
    "objectID": "design-external.html",
    "href": "design-external.html",
    "title": "4  Design External to the School",
    "section": "",
    "text": "Contains a overview of the perspective from industry or others outside of academia on the Design space of Data Science."
  },
  {
    "objectID": "design-case-study.html",
    "href": "design-case-study.html",
    "title": "5  Design Case Study",
    "section": "",
    "text": "Overview of the Design case study for the current semester for the Foundations of Data Science course."
  },
  {
    "objectID": "value-lab.html",
    "href": "value-lab.html",
    "title": "7  Labs Rubric - Guess Who",
    "section": "",
    "text": "Individual Assignment\nGeneral Descripition: This lab is design for you to reflect on the process used for winning the game Guess Who. Students should work in teams to play the game and take notes on the processes that work best for quickly guessing the opponents card. This will likely include the creation of a tree based decision diagram, a example will be provided in class.\nPreparatory Assignments - None\nWhy am I doing this? In order to allow you to reflect on how data is used to generated decision/predictive algorithms and what issues could arise as a result. Work to incorporate the materials you’ve been exposed to up to this point as you work through the lab.\nWhat am I going to do? You are working through a interactive process with your group to find the optimal list of yes/no questions for any given card. Play the game as many times as necessary to construct a list of the top five questions using a tree based diagram.\nAnswer these questions:\n\nIn this situation what is the dataset and what is the algorithm?\nHow did your approach to identifying cards change throughout the lab period?\nAre there cards that are easier to identify, why?\nThis is just a game, but given the materials you’ve been exposed to what concerns do you have about this process and algorithmic decision making?\nWhich of the areas of “Value” presented in class could help to solve the issues you identified in question 4? (What Tool)\n\nTips for success:\n\nDon’t worry about winning\nWork as a team with your group\nTry to document the process used right at the start of the lab, don’t wait till after you have played to start taking notes\n\nHow will I know I have succeeded\n\n\n\n\n\n\n\nSpecs Category\nSpecs Details\n\n\n\n\nFormatting\n\nSubmit Via Canvas\nText answers to questions\n\n\n\nText\n\nGoal: The questions are designed to be answer during or right after the lab period.\nUse a few sentences to answer each question in Canvas\n\n\n\n\nAcknowledgements: Special thanks for Jess Taggart from UVA CTE for coaching us. This structure is pulled directory from Steifer & Palmer (2020)."
  },
  {
    "objectID": "value-lab-II.html",
    "href": "value-lab-II.html",
    "title": "8  Labs Rubric - AI Fairness 360",
    "section": "",
    "text": "Individual Assignment\nGeneral Descripition: This lab is designed for you to get exposure to AI fairness approaches in a no code environment on the website AI Fairness 360. You will be able to work through the various fairness methods at different stages of the pipeline and reflect on which methods seem to work the best on the given datasets.\nPreparatory Assignments - None\nWhy am I doing this? In order to give you exposure to and practice with the various methods being developed and deployed in the ML fairness space. After completing the lab you’ll have a better sense of how these tools are used, when they are used and how the work.\nWhat am I going to do? The AI Fairness 360 website has a demo module that includes three datasets. Work through the demo on all three datasets, trying all the methods provided, and answer the questions below.\nAnswer these questions:\n\nFor each protected class variable which evaluation methods showed bias to be present?\nNote how each method preform at removing bias.\nWas the accuracy of the model effected when using the various approaches, if so how?\nGiven the above what are some patterns you noticed, which methods seem to work the best, where in the data process are these methods located (pre/in/post).\n\nTips for success:\n\nTake careful notes as you go through each method\nHave fun\n\nHow will I know I have succeeded:\n\n\n\n\n\n\n\nSpecs Category\nSpecs Details\n\n\n\n\nFormatting\n\nSubmit Via Canvas\nText answers to questions\n\n\n\nText\n\nGoal: The questions are designed to be answer during or right after the lab period.\nBullett points are fine for the first three questions, paragraph from for the fourth\n\n\n\n\nAcknowledgements: Special thanks for Jess Taggart from UVA CTE for coaching us. This structure is pulled directory from Steifer & Palmer (2020)."
  },
  {
    "objectID": "value-lab-III.html",
    "href": "value-lab-III.html",
    "title": "9  Labs Rubric - Explainable AI Interview",
    "section": "",
    "text": "Individual Assignment\nGeneral Descripition: This lab is designed for you to verbalize the concept of Explainable AI to someone that is unfamiliar with the concepts and track their responses.\nPreparatory Assignments - None\nWhy am I doing this? This will allow you to work on communicating the concepts thus understanding them better and judging whether trust is increased with the knowledge that explainable approach to AI models are present.\nWhat am I going to do? Interview three people that have somewhat limit knowledge of Machine Learning and AI. Ask the first two questions then provide a brief overview of Explainable AI and then ask the remaining three questions. After all three interviews are complete provide a brief summary of what patterns you notice (a paragraph or two is fine)\nAnswer these questions:\n\nDo you believe that AI has a large influence over society currently?\nOn a scale from 1 to 10, with 10 being total trust and 1 being no trust: Do you believe that AI models can be trusted and used in a way that promotes positive outcomes for individuals and society?\nGive a brief overview of the Data Ethic concepts with a emphasis on Explainable AI models.\nGiven what you just heard does that in anyway change your answer to question 2, such that, if explainable AI models were more commonly used would you be more likely to believe that they can promote positive outcomes for individuals and society? (scale of 1 to 10 for you belief the AI models can promote positive outcomes)\nAre you interested in learning more about Explainable AI as a concept?\n\nTips for success:\n\nKeep your over of data Ethics and Explainable AI at a high level and use the slides provided in class as needed.\nFeel free to improvise and ask additional questions if you’d like or change the questions slightly.\n\nHow will I know I have succeeded:\n\n\n\n\n\n\n\nSpecs Category\nSpecs Details\n\n\n\n\nFormatting\n\nSubmit Via Canvas\nSummary of your three interviews in one document\nA reflection on any patterns you noticed in the answers\n\n\n\nText\n\nGoal: The interview is designed to be fairly quick, 10 minutes or so for each. All together the assignment should take approximately one hour.\n\n\n\n\nAcknowledgements: Special thanks for Jess Taggart from UVA CTE for coaching us. This structure is pulled directory from Steifer & Palmer (2020)."
  },
  {
    "objectID": "value-case-study.html",
    "href": "value-case-study.html",
    "title": "11  Value Case Study",
    "section": "",
    "text": "of this Value domain"
  },
  {
    "objectID": "analytics-sds.html",
    "href": "analytics-sds.html",
    "title": "16  Analytics UVA DS",
    "section": "",
    "text": "Code\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr=np.arange(0,2,.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'}\n)\nax.plot(theta,r)\nax.set_rticks([.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\nFigure 16.1: A line plot on a polar axis"
  },
  {
    "objectID": "analytics-lab.html",
    "href": "analytics-lab.html",
    "title": "Defining Data Science",
    "section": "",
    "text": "DS 1001 - Spring 2023 - Professors Wright and Alonzi\nDue: End of lab period (or later that day)\nSubmission format: Word doc or PDF summarizing your findings\nIndividual Assignment\nGeneral Descripition: This lab is designed to introduce the basic concepts behind information gain and entropy. Mastermind is a game were through a series of questions one player tries to determine a “code” created by another player. In doing so the value of asking questions that provide a high level of information will become paramount.\nPreparatory Assignments - None\nWhy am I doing this? In order to gain a better understanding of information gain and entropy through hands on experience.\nWhat am I going to do? Work with a partner or group of three and alternate playing the game. At the onset of each game the code-maker should calculate the entropy of the chosen pegs for the “code”. The code-breaker should note the results of the game . This should include whether the code was broken or not and the number of rounds used. Try to be the code-maker and breaker at least 3 times. The code-maker should intentionally use very different types of “code”, this should impact how the game is played.\n\nNote the range of entropy for binary cases is 0 to 1 for more than 2 classes it is 0 to log2 k, where k is the number of classes.\n\nAnswer these questions:\n\nWhat combinations of pegs (code) seemed to be harder to break?\nDid your approach to asking questions change as you played?\nDescribe where in the game information gain is being presented?\n\nTips for success:\n\nDon’t worry about winning, instead think about what is happening during game play.\nWork as a team with your group.\nTry to document the process used right at the start of the lab, don’t wait till after you have played to start taking notes.\n\nHow will I know I have succeeded:\n\n\n\n\n\n\n\nSpecs Category\nSpecs Detail\n\n\n\n\nSubmission\nSubmit via Canvas a PDF or Word Document\n\n\nText\n* Answer the above questions * When you were the code-maker submit the Entropy * When you were the code-breaker submit the results\n\n\n\nAcknowledgements: Special thanks for Jess Taggart from UVA CTE for coaching us. This structure is pulled directory from Steifer & Palmer (2020)."
  },
  {
    "objectID": "appendix-sources.html",
    "href": "appendix-sources.html",
    "title": "20  Primary Sources",
    "section": "",
    "text": "The primary sources on which the conclusions of this essay are based comprise a variety of documents, from technical journals to blog posts to internal reports. They come from a range of viewpoints, from data analysis and statistics to data mining and data science per se. For the purposes of the essay, we select a more or less representative subset across these axes of variation. With respect to representativeness, in some cases a document was chosen for its influence, in others, such as the post by Dataman, because it is considered more or less typical of a common genre.\nThe documents chosen are listed below in chronological order, beginning with Tukey’s seminal essay on data analysis and ending with contempory explainers. Included also are the definitions of the CRISP-DM and KDD processes which are the most developed pipeline models.\nEach source entry below contains a short description of the source and its context, and then a list of the phases cited by the authors as fundamental to data processing. These phases are also mapped onto the standard sequence described in the main part of this essay, listed here for convenience.\n\nUnderstand\nPlan\nCollect\nStore\nClean\nExplore\nPrepare\nModel\nInterpret\nCommunicate\nDeploy\nReflect\n\nMappings are indicated by an arrow pointing to the subset of terms from the standard sequence, e.g. … \\(\\rightarrow [Explore]\\) These mappings are also aggregated into a composite pipeline and displayed the table below; each model row is referenced by its key as defined in the entries.\nNote that in most cases these phases are explicitly described as a process and often as a pipeline. When they are not, the implication is strong. In some cases, the process is likened to a cycle, emphasizing the connection between the endpoints of the pipeline, which is also emphasized by the 4+1 model.\nA final feature added to each entry is a two-value indicator of bias — statistics and data mining. This is meant to capture the intellectual origin of the model, given that statistics and data mining define the poles of one of main axes of variance that defines the field of data science. This difference roughly corresponds to the “two cultures” described by Breiman Breiman (2001).\n\n\n\n\nKey: Tukey\nYear: 1962\nSource: Tukey (1962) URL\nBias: Statistics\nIn this classic essay, Tukey introduces the concept of data analysis, which he distinguishes from mathematical statistics and likens to an empirical science. He defines data analysis as an empirical process with phases including “… procedures for analyzing data, techniques for interpreting the results of such procedures, ways of planning the gathering of data to make its analysis easier, more precise or more accurate, and all the machinery and results of (mathematical) statistics which apply to analyzing data” (p. 2). Unpacking this statement yields a four phase model.\n\nPlanning: This phase includes “ways of planning the the gather of data to make its analysis easier.” \\(\\rightarrow [Plan]\\)\nGathering: The gathering of data, either through creation or by acquisition of “data already obtained” (p. 40). Includes also the shaping of data “to make its analysis easier,” which corresponds to our concept of Preparation. \\(\\rightarrow [Collect, Prepare]\\)\nAnalyzing: This is where data are analyzed with “all the machinery and results of (mathematical) statistics.” \\(\\rightarrow [Explore, Model]\\)\nInterpreting: “techniques for interpreting the results of” analysis. \\(\\rightarrow [Interpret]\\)\n\n\n\n\nKey: KDD\nYear: 1996\nSource: Fayyad et al. (1996) URL→\nBias: Data Mining\n\nKDD, or Knowledge Discovery in Databases, emerged in the late 1980s as both datasets and the computational resources to work with them became abundant. These resources included commercial databases and personal computers. In many ways the most adjacent field to contemoporary data science, this approach is unabashedly dedicated to finding patterns in data prior to developing a probabilistic model to justify their use. Fayyad’s essay identifies five steps (Fayyad et al. 1996: 84). He emphasizes the highly iterative and cyclical nature of the process, arguing that it “may contain loops between any two steps.” Another significant aspect of this conception of the pipeline is the role of exploration in the analytical phase: “Data Mining is a step in the KDD process consisting of applying data analysis and discovery algorithms that, under acceptable computational efficiency limitations, produce a particular enumeration of patterns over the data ….” (p. 83)\n\nSelection: Creating a target data set, or focusing on a subset of variables or data samples, on which discovery is to be performed. \\(\\rightarrow [Collect]\\)\nPre-processing: Cleaning and pre processing the data in order to obtain consistent data. \\(\\rightarrow [Clean]\\)\nTransformation: Transformation of the data using dimensionality reduction and other methods. \\(\\rightarrow [Prepare]\\)\nData Mining: Searching for patterns of interest in a particular representational form, depending on the DM objective (usually, prediction). \\(\\rightarrow [Model]\\)\nInterpretation/Evaluation: Interpretation and evaluation of the mined patterns. \\(\\rightarrow [Interpret]\\)\n\n\n\n\nKey: SEMMA\nYear: 1996\nSource: Azevedo and Santos (2008)\nBias: Statistics\nThe SEMMA model was developed the by SAS institute in 1996 as part of the documentation for their product, SAS Enterprise Miner. Even so, the model is referenced outside of this context, often as a comparison to KDD and CRISP-DM. Its bias towards statististics is evident in the first step.\n\nSample: Sampling the data by extracting a portion of a large data set big enough to contain the significant information, yet small enough to manipulate quickly. \\(\\rightarrow [Collect]\\)\nExplore: Exploration of the data by searching for unanticipated trends and anomalies in order to gain understanding and ideas \\(\\rightarrow [Explore]\\)\nModify: Modification of the data by creating, selecting, and transforming the variables to focus the model selection process \\(\\rightarrow [Prepare]\\)\nModel: Modeling the data by allowing the software to search automatically for a combination of data that reliably predicts a desired outcome. \\(\\rightarrow [Model]\\)\nAssess: Assessing the data by evaluating the usefulness and reliability of the findings from the DM process and estimate how well it performs. \\(\\rightarrow [Interpret]\\)\n\n\n\n\nKey: Hayashi\nYear: 1998\nSource: Hayashi et al. (1998) URL→\nBias: Statistics\nThe Japanese statistician Chikio Hayashi adopted the term “data science” in the early 1990s to define a field that did not succumb to what he saw to be the errors of both statistics and data analysis. He argued that mathematical statistics had become too attached to problems of inference and removed from reality, while data analysis had lost interest in understanding the meaning of the data it deals with. His definition of data science is decidely processual: “Data Science consists of three phases: design for data, collection of data and analysis on data. It is important that the three phases are treated with the concept of unification based on the fundamental philosophy of science …. In these phases the methods which are fitted for the object and are valid, must be studied with a good perspective.” (p. 41) Similar to KDD and CRISM-PM, Hayashi envisioned this process as a spiral, oscillating between poles if what he called “diversification” and “simplification.” Note also that each of these terms, as described, comprises more than on of the standard sequence phases.\n\nDesign: Surveys and experiments are developed to capture data from “multifarious phenomena.” \\(\\rightarrow [Understand, Plan]\\)\nCollection: Phenomena are expressed as multidimensional or time-series data; properties of the data are made clear. At this stage, data are too complicated to draw clear conclusions. (Representation) \\(\\rightarrow [Collect, Explore, Prepare]\\)\nAnalysis: By methods of classification, multidimensional data analysis, and statistics, data structure is revealed. Simplification and conceptualization. Also yields understanding of deviations of the model, which begins the cycle anew. (Revelation) \\(\\rightarrow [Model, Interpet]\\)\n\n\n\n\nKey: CRISPDM\nYear: 1999\nSource: Wirth and Hipp (1999) URL→\nBias: Data Mining\nBy the late 1990s, the practice of data mining had become widespread in industry and globally. In 1999 the Cross Industry Standard Process for Data Mining (CRISP-DM) was developed in Europe as a comprehensive and general model to support the use of data mining in a broad range of sectors in a principled manner. Designed to work within a project management framework, this model is by far the most developed, and it continues to influence the field of data science to this day. Like KDD before it, the model emphasizes the cyclic and recursive nature of the process, and this perspective is reflected in the circular diagram that often accompanies its presentation. The steps below are based on the summary presented in Wirth and Hipp’s essay.\n\n\n\nProcess diagram showing the relationship between the different phases of CRISP-DM (Wikipedia)\n\n\n\nBusiness Understanding: Understanding project objectives and requirements from a business perspective. Includes the development of a plan. \\(\\rightarrow [Understand, Plan]\\)\nData Understanding: The initial data collection and activities to get familiar with the data, e.g. to identify data quality problems, to discover first insights into the data, or to detect interesting subsets to form hypotheses for hidden information. This is really two phases — Collection and Exploration — which are combined because of their close, iterative relationship. \\(\\rightarrow [Collect, Explore]\\)\nData Preparation: Construction of the final dataset for analytical use. Tasks include table, record, an attribute selection, data cleaning, construction of new attributes, and transformation of data for modeling tools. \\(\\rightarrow [Clean, Prepare]\\)\nModeling: Modeling techniques are selected and applied, parameters calibrated. Modeling techniques include a broad range of unsupervised and supervised methods. As with KDD, there is an emphasis on pattern discovery, which has the effect of promoted methods that other models place squarely in the Explore phase of the standard sequence. \\(\\rightarrow [Model]\\)\nEvaluation: Evaluation of model performance by both intrinsic and extrinsic measures. Regarding the latter, a key objective is to determine if an important business issue has not been sufficiently considered. \\(\\rightarrow [Interpret]\\)\nDeployment: The knowledge gained by the model is presented in a way that the customer can use it. This may be something as simple as a report or as complex as a repeatable data mining process. In many cases the user, not the data analyst, will carry out the deployment. \\(\\rightarrow [Deploy]\\)\n\n\n\n\nKey: OSEMI\nYear: 2010\nSource: Mason and Wiggins (2010) URL→\nBias: Data Mining\nAfter the phrase “data science” went viral (circa 2009), there were many efforts to make sense of the idea. In 2010 Drew Conway posted his Venn diagram of data science (Conway 2010). The same year, another influential model, based explicitly on the pipeline, came from Mason and Wiggins in a blog post hosted at O’Reilly’s Tech Radar site. In contrast to previous models rooted in statistics, this model assumes that data are abundant and available, such as data scrapable from the Web.\n\nObtain: Gather data from relevant sources through APIs, web scraping, etc. \\(\\rightarrow [Collect]\\)\nScrub: Clean data and convert data to machine readable formats. Clearning includes handling missing data, inconsistent labels, or awkward formatting; stripping extraneous characters; normalizing values, etc. \\(\\rightarrow [Clean, Prepare]\\)\nExplore: Find significant patterns and trends using statistical and data analytic methods, such as visualizing, clustering. Also includes transformations of the for more effective analysis, such as dimensionality reduction. \\(\\rightarrow [Explore]\\)\nModel: Construct methods to predict and forecast. These methods include those of inferential statistics and predictive machine learning. \\(\\rightarrow [Model]\\)\nInterpret: Making sense of the results as well as evaluating the performance of models. May involve domain experts. Also includes methods such as regularization that make models interpretable to those who use them, e.g. scientists or business people. \\(\\rightarrow [Interpret]\\)\n\n\n\n\nKey: Ojeda+\nYear: 2014\nSource: Ojeda et al. (2014) URL→\nBias: Data Mining\nBy 2014, data science had become a widespread practice in industry and the academic, and explanations of its nature became the subject of many books. This text is one of a genre that presents the field as a process, perhaps due to the influence of the CRISP-DM and OSEMI models, and uses the expression pipeline throughout. Note that the model defined in this book is not presented here as canonical. It suffers from various inconsistences, such as the labeling of steps in the text representation of the pipeline versus those on diagrams. It is included to demonstrate the pervasiveness of the model.\n\nAcquisition: Acquire the data from relational databases, NoSQL and document stores, web scraping, distributed databases (e.g. HDFS on a Hadoop platform), RESTful APIs, flat files, etc. Consistent with the other data mining models, the emphasis here is on working with available data, not generating it. \\(\\rightarrow [Collect]\\)\nExploration and understanding: Understand the data and how it was collected or produced; this often requires significant exploration. Note that this step does not correspond to exploration in the sense of exploratory data analysis (EDA). Rather, it reflects the position of the data scientist as the receiver of someone else’s data and the need to infer what would normally belong to the first step of the standard squence \\(Understand\\). \\(\\rightarrow [Understand]\\)\nMunging, wrangling, and manipulation: Convert the data into the form required for analysis. This includes a wide range of activities, such as those mentioned in previous models. However, it also conflates the standard phases \\(Clean\\) and \\(Prepare\\). \\(\\rightarrow [Clean, Prepare]\\)\nAnalysis and modeling: Apply statistical and machine learning methods, including clustering, categorization, and classification. One presumes that the standard step of \\(Explore\\) is included here. \\(\\rightarrow [Explore, Model]\\)\nCommunicating and operationalizing: At the end of the pipeline, we need to give the data back in a compelling form and structure, sometimes to ourselves to inform the next iteration, and sometimes to a completely different audience. The data products produced can be a simple one-off report or a scalable web product that will be used interactively by millions. \\(\\rightarrow [Communicate, Deploy]\\)\n\n\n\n\nKey: Caffo+\nYear: 2015\nSource: Caffo, Peng, and Leek (2015) URL→\nBias: Statistics\nBy 2015, many universities had begun offering degrees in data science, typically at the masters’ level, with the intention of meeting the high demand for data scientists. Professors Caffo, Peng, and Leek’s book was written to accompany a course in Exectutive Data Science, offered by Johns Hopkins University through Coursera. Their model is relatively high level, consisting of five phases, given the target audience of those in charge of data science teams. As with other models, this model emphasizes the iterative nature of each phase, both internally and between phases. And as with many statistics-oriented conceptions of data science, this model emphasizes the Understand phase and skips over the technical issues of storing and modeling the data.\n\nQuestion.: Pose a research question and specify what is to be learned from data to answer it. The question determines the data to be obtained and the type of analysis to perform. Included determing the type of question, including descriptive, exploratory, inferential, causal, predictive, and mechanistic. An alternate approach here is hypothesis generation, which may be suitable when data already exist but a question is not well-developed. In this scenario, the data scientist may skip to the next step to determine the value of the data. Once a question is developed, then it may be necessary to acquire more data, and then go through the process. \\(\\rightarrow [Question, Collect]\\)\nExploratory data analysis: Explore the data to determine if the data are suitable for answering the question and if more data need to be collected. For example, determine if there are enough data and if it is missing key variables. In addition, develop a sketch of the solution. Include a freamework for challenging results and to develop robust evidence for answering your question.  \\(\\rightarrow [Explore]\\)\nFormal modeling: Identify the parameters to estimate based on the sketch.   \\(\\rightarrow [Model]\\)\nInterpretation: Determine if the modeling results align with the initial expections during the Question phase and before the acquisition of data. Consider the totality of the evidence developed after attempting to fit different models, weighing the different pieces of evidence.   \\(\\rightarrow [Interpret]\\)\nCommunication: Communicate findings to various audiences, either internal to the organization or external. Includes translating findings into action by virtue of effectively communicating results to decision-makers. \\(\\rightarrow [Communicate]\\)\n\n\n\n\nKey: Donoho\nYear: 2017\nSource: Donoho (2017) URL→\nBias: Statistics\nAs data science became viral in the 2010s, academic statisticians frequently expressed concern that they were “disconnected from the new (and vaguely defined) community of data scientists, who are completely identified with Big Data in the eyes of the media and policymakers” (Rodriguez 2012). “Aren’t We Data Scientists?” asked Marie Davidian, then president of the American Statistical Association, in 2013 (Davidian 2013). In response to this growing sentiment, Donoho’s essay reads as a manifesto for the reclaiming of data science by academic statistics. In it, he defines six divisions of Greater Data Science, each containing a set of subactivities that roughly map to the pipeline model described here.\nIt is important to note that Donoho’s model is more abstract than a pipeline description and therefore not all of the divisions and subactivities directly map onto the sequence. Data visualization and Presentation defines a general practice, although from the description it clearly maps onto two phases, Explore and Communicate. Computing with Data refers to knowledge of programming languages for data analysis and data processing as well as knowledge of how to use cluster and cloud computing resources at scale. It also includes how to develop workflows which organize work. Clearly, this area belongs to no phase in particular but instead characterizes the broader context in which the data science pipeline operates. The identification of workflows, which are the focus of the Science about Data Science division, also suggests that Donoho is working at a higher level of abstraction than the other models, which places it alongside the of the current essay. The following phases are inferred from Donoho’s descriptions.\n\nGathering: This includes both experimental design, modern data gathering techniques, and identification of existing data resources, from signal data to websites. \\(\\rightarrow [Plan, Collect]\\)\nPreparation: Identification of anomalies and artifacts in the data and handling them by reformatting, recoding, grouping, smoothing, subsetting, etc. \\(\\rightarrow [Clean]\\)\nExploration: Application of EDA to sanity-check data and expose unexpected features. Includes data visualization, which Donoho separates out into a separate division and combines with visualization activities involved in interpretation and communication. \\(\\rightarrow [Explore]\\)\nModern Databases: Transform and restructure data as found in source files, such as CSV files and spreadsheets, and databases, into a forms more suitable for analysis. \\(\\rightarrow [Prepare]\\)\nMathematical Representations: Application of mathematical structures for to extract features from special kinds of data, including acoustic, image, sensor, and network data. For example, the application of the Fourier transform to acousting data or the wavelet transform to image data. \\(\\rightarrow [Prepare]\\)\nData Modeling: Appliction of methods from both traditional statistics and contemporary machine learning. \\(\\rightarrow [Model]\\)\nPresentation: The creation of sophisticated graphics, dashboards, and visualizations to present conclusions to stakeholders. \\(\\rightarrow [Communicate]\\)\nScience about Data Science: In the spirit of Tukey’s “science of data analysis,” this is the evaluation of what data scientists actually do and produce. Includes the identificatin and study of commonly occurring analytical and processing workflows. \\(\\rightarrow [Reflect]\\)\n\n\n\n\nKey: Géron\nYear: 2017\nSource: Géron (2017) URL→\nBias: Data Mining\nGéron’s text is a classic among practicing data scientists interested in using machine learning in a business setting, covering everything from regression to deep learning from a practical, code-centric perspective. Written with “minimal theory,” the book demostrates the entrenched nature of the pipeline model, especially as it has been become a software development pattern hard-coded into both SciKit Learn and TensorFlow. This usage reflects the fact that within machine learning, “pipeline” has taken on a more specific meaning — “a sequence of data processing components” — than we are using here. These components are units software within a system, not the phases of labor associated with the work of the data scientist. Nevertheless, Géron’s text describes a labor pipeline within which the software pipeline is embedded, the steps of an “end-to-end” classification project.\n\nLook at the big picture: Frame the problem by defining the business objective and the specific goals of the model. This may include defining a specific performance measure, such as a loss function. Consider that the model is a a means to an end. \\(\\rightarrow [Understand]\\)\nGet the data: This consists of setting up a compute workspace and downloading the data. This step also includes getting to know the data and preparing a test set. \\(\\rightarrow [Get, Prepare]\\)\nDiscover and visualize the data to gain insights: Go into more depth exploring the data, using EDA methods to investigate correlations and experiment with attribute combinations. \\(\\rightarrow [Explore]\\)\nPrepare the data for Machine Learning algorithms: This step involves transforming and structuring the data in forms suitable for the algorithms that with fit the data to a model. This includes imputing missing data, handling non-numeric data, feature scaling, etc. This step contains its own pipeline. \\(\\rightarrow [Clean, Prepare]\\)\nSelect a model and train it: Apply models deemed appropriate to the data and compare results. Apply evaluation methods such as cross-validation to compare model results. \\(\\rightarrow [Model]\\)\nFine-tune your model: Once the list of candidate models is shortened, fine-tune their parameters by using various seach methods, e.g. grid, randomized, or ensemble. Also includes evaluating the models on test sets. \\(\\rightarrow [Model]\\)\nPresent your solution: : This step includes presenting to stakeholders what was learned, what worked and what did not, what assumptions were made, and what the system’s limitations are. It also includes documenting everything, creating user-friendly presentations with clear visualizations and easy-to-remember statements. Géron refers to this as the “prelaunch phase,” presumably because the component must be approved to go on to the next phase. \\(\\rightarrow [Communicate]\\)\nLaunch, monitor, and maintain your system: This step includes converting your model into a production-ready component that can become a functioning piece of the overall pipeline. This may mean creating a web service. \\(\\rightarrow [Deploy]\\)\n\n\n\n\nKey: Das\nYear: 2019\nSource: Das (2019) URL→\nBias: Data Mining\nThis essay belongs to a genre of self-publication that attempts to explain concepts in data science to the public. It is typical of platforms like Medium and what used to be called the blogosphere. It is included here to represent the commonplace nature of the pipeline as a rhetorical device for explaining data science. Here, the pipeline is called a “life-cycle,” although the term pipeline is used as well. The cyclical nature of the process is emphasized by including the first step as last step of the process. [Note that this essay was removed from the web by the author; a link to Internet Archive URL is included for completeness.]\n\nBusiness Understanding: Understand the problem you are trying to solve and how data can be used to support a solution or decision. Identify central objectives and variables that need to be predicted. (Here the author implies that methods such as regression and clustering are objectives.) \\(\\rightarrow [Understand]\\)\nData Mining: Gathering the data from various sources. This may invovle extracting data from a legacy database or webscraping. The author correctly notes that this step should not be lumped together with cleaning. \\(\\rightarrow [Collect]\\)\nData Cleaning: This step includes cleaning and preparing the data, also know as “data janitor work.” This step takes most of the data scientist’s time because there are so many reasons that data may need cleaning. Also includes handling missing data.\n\\(\\rightarrow [Clean]\\)\nData Exploration: This is the brainstorming phase of data analysis, where one discovers patterns and biases in the data. Involves using the basic tools of EDA but also creating interactive visualizations to allow drilling down into specific points, e.g. to explore the story behind outliers. Here also one begins to form hypotheses about the data to be developed. \\(\\rightarrow [Explore]\\)\nFeature Engineering: This is the process of using domain knowledge to transform the data into informative features that represent the business problem. This stage directly influences the accuracy of the predictive model constructed in the next stage. Methods include feature selection (i.e. dimensionality reduction) and constructing new features that will aid in the modeling process. \\(\\rightarrow [Prepare]\\)\nPredictive Modeling: The application of machine learning methods to the data. Includes training several models and evaluating their performance, as well as applying statistical methods and tests to ensure that the outcomes from the models make sense and are significant. Based on the questions developed in the business understanding stage, this is where a model is selected. Model selection will depend on the size, type and quality of the data, availability of computational resources, and the type of output required. \\(\\rightarrow [Model]\\)\nData Visualization: This step combines expertise from the fields of communication, psychology, statistics, and art, with an ultimate goal of communicating the insightw from the model in a simple yet effective and visually pleasing way. \\(\\rightarrow [Communicate]\\)\n\n\n\n\nKey: Dataman\nYear: 2020\nSource: Dataman (2020) URL→\nBias: Data Mining\nAnother example of a self-published explainer essay, this one describes the data science “modeling process” and aligns it with six consultative roles. The other defines eight steps to the process. Curiously, althhough this pipeline focuses on the details of training models, it does not include training the model itself as a step.\n\nSet the objectives: This step includes defining the goals of the model as well as its scope and risk factors. These will determine what data to collect, and whether the cost to collect the data can be justified by the impact of the model. \\(\\rightarrow [Understand]\\)\nCommunicate with key stakeholders: This step involves ongoing aligning expected outcomes with key stakeholders. This step is unique among the pipelines by being place so early in the process. We associate it with the \\(Understand\\) phase because it essentially broads the group for whom understanding matters. \\(\\rightarrow [Plan]\\)\nCollect the necessary data for exploratory data analysis (EDA): This step combines the \\(Collect\\), \\(Clean\\), and \\(Explore\\) phases. Involves the iterative “curation” of data need to conduct EDA. \\(\\rightarrow [Collect, Clean, Explore]\\)\nDetermine the functional form of the model: In this step, the specific from of the model is defined, including the definition and characterization of the target variable. This step involves model selection and would in practice be closely associated with the next. \\(\\rightarrow [Prepare, Model]\\)\nSplit the data into training and validation This step is concerned with model validation and avoiding overfitting. Data are divided into training and test datasets. It is assumed that test data were separated out prior to the preceding step. Presumably this step includes fitting the models, but this is not explicit. \\(\\rightarrow [Prepare, Model]\\)\nAssess the model performance: This step includes determining the stability of a model over time (generalizability), focusing on the overall fit of the model, the significance of each predictor, and the relationship between the target variable and each predictor. Includes measures such as lift. Clearly this step follows the process of fitting and tuning models. \\(\\rightarrow [Model]\\)\nDeploy the model for real-time prediction: The deployment of machine learning models into production, e.g. via batch prediction as a webservice. \\(\\rightarrow [Deploy]\\)\nRe-build the model: This step involves revisiting the pipeline as models lose their predictability due to a variety of causes. Effectively, this step asserts the cyclic and interative nature of the pipeline and therefore belongs to no step in particular.\n\n\n\n\nKey: Porter\nYear: 2020\nSource: Porter (2020)\nBias: Statistics \nMichael Porter is an Associate Professor of Data Science and Systems Engineering at the UVA. This essay is an internal report (available on request) on the field of data science from the perspective of curricular planning. Porter argues that Data Science includes seven areas, each of which can be viewed as a science, i.e. as requiring specific expertise. Like Donoho’s essay (and the current), the model presented is more abstract than a pipeline model and includes areas that cross-cut steps in the Primary Sequence. Nevertheless, it retains a sequential structure consistent with the general pattern.\n\nData Collection and Acquisition: The science of “how” and “when” data is collected, and includes all methods of data acquisition from its production through designed experiments to its consumption from external sources, e.g. databases and APIs. \\(\\rightarrow [Collect]\\)\nData Storage and Representation: The science of “how” and “when” data is collected, including data modeling and storing data in databases and files of various formats. Also includes transforming data into “tidy” format. \\(\\rightarrow [Store]\\)\nData Manipulation and Transformation: The science of preparing data for analysis, including wrangling, cleaning, and importing data from databases (after they have been stored in the previous step). \\(\\rightarrow [Clean, Prepare]\\)\nComputing with Data: The science of computing for data analysis with a focus on algorithm design and performance evaluation. \\(\\rightarrow [Model]\\)\nData Analytics: The science of machine learning, broadly conceived to include methods ranging from geneative modeling (either frequentist or Bayesian) and inference to predictive modeling and optimization. Notably, this step also includes EDA and feature engineering. \\(\\rightarrow [Explore, Prepare, Model]\\)\nSummarizing and Communicating Data and Models: The science of extracting and summarizing the information in data for human understanding. This area includes visualization in the context of both EDA and presentation of results to external stakeholders. It also includes the communication of model and data properties (such as bias) to guide interpretation of results. Here we map it to the latter, but note that this area includes at least two steps. In addition, we may may the work of summarization to interpretation. \\(\\rightarrow [Interpret, Communicate]\\)\nPracticing Data Science: The science of the overall system of data science, including improving the data science spipeline, replicability of results, openness and transparency, project management, etc. \\(\\rightarrow [Reflect]\\)\nDisciplinary Data Science: The science of applying data science to specific disciplines. This involves a consideration of how the pipeline operates in different contexts, including how domain knowledge informs each of the steps of the pipeline, from mode of data acquisition to model selection and analytic appraoch, to the interpretation and communication of results. Although placed at the end of the list, it properly belongs to the initial \\(Understand\\) step. \\(\\rightarrow [Understand]\\)\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n      \n      understand\n      plan\n      collect\n      store\n      clean\n      explore\n      prepare\n      model\n      interpret\n      communicate\n      deploy\n      reflect\n    \n    \n      \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n    \n  \n  \n    \n      Tukey\n      \n      ■\n      ■\n      \n      \n      ■\n      ■\n      ■\n      ■\n      \n      \n      \n    \n    \n      KDD\n      \n      \n      ■\n      \n      ■\n      \n      ■\n      ■\n      ■\n      \n      \n      \n    \n    \n      SEMMA\n      \n      \n      ■\n      \n      \n      ■\n      ■\n      ■\n      ■\n      \n      \n      \n    \n    \n      Hayashi\n      ■\n      ■\n      ■\n      ■\n      \n      ■\n      ■\n      ■\n      ■\n      \n      \n      \n    \n    \n      CRISPDM\n      ■\n      ■\n      ■\n      ■\n      ■\n      ■\n      ■\n      ■\n      ■\n      ■\n      ■\n      \n    \n    \n      OSEMI\n      \n      \n      ■\n      \n      ■\n      ■\n      ■\n      ■\n      ■\n      \n      \n      \n    \n    \n      Ojeda+\n      ■\n      \n      ■\n      ■\n      ■\n      \n      ■\n      ■\n      \n      ■\n      ■\n      \n    \n    \n      Caffo+\n      ■\n      ■\n      ■\n      \n      \n      ■\n      \n      ■\n      ■\n      ■\n      \n      \n    \n    \n      Donoho\n      \n      ■\n      ■\n      \n      ■\n      ■\n      ■\n      ■\n      \n      ■\n      \n      ■\n    \n    \n      Géron\n      ■\n      \n      ■\n      \n      ■\n      ■\n      ■\n      ■\n      \n      ■\n      ■\n      \n    \n    \n      Das\n      ■\n      \n      ■\n      \n      ■\n      ■\n      ■\n      ■\n      \n      ■\n      \n      \n    \n    \n      Dataman\n      ■\n      \n      ■\n      \n      ■\n      ■\n      ■\n      ■\n      \n      \n      ■\n      \n    \n    \n      Porter\n      ■\n      \n      ■\n      ■\n      ■\n      \n      ■\n      ■\n      ■\n      ■\n      \n      ■\n    \n  \n\n\n\n\n\n\n\n\nAzevedo, Ana Isabel Rojão Lourenço, and Manuel Filipe Santos. 2008. “KDD, SEMMA and CRISP-DM: A Parallel Overview.” IADS-DM.\n\n\nBreiman, Leo. 2001. “Statistical Modeling: The Two Cultures.” Statistical Science 16 (3): 199–231. https://doi.org/10.1214/ss/1009213726.\n\n\nCaffo, Brian, Roger D. Peng, and Jeffrey Leek. 2015. Executive Data Science. Leanpub.\n\n\nConway, Drew. 2010. “The Data Science Venn Diagram.” Drew Conway.\n\n\nDas, Sangeet Moy. 2019. “Data Science Life Cycle 101 for Dummies Like Me.” Medium. https://web.archive.org/web/20191113225625/https://towardsdatascience.com/data-science-life-cycle-101-for-dummies-like-me-e66b47ad8d8f?gi=261acdd4c903.\n\n\nDataman, Dr. 2020. “Data Science Modeling Process & Six Consultative Roles.” Medium. https://towardsdatascience.com/data-science-modeling-process-fa6e8e45bf02.\n\n\nDavidian, Marie. 2013. “Aren’t We Data Science?” AMSTAT News: The Membership Magazine of the American Statistical Association, no. 433: 3.\n\n\nDonoho, David. 2017. “50 Years of Data Science.” Journal of Computational and Graphical Statistics 26 (4): 745–66. https://doi.org/10.1080/10618600.2017.1384734.\n\n\nFayyad, Usama M, Gregory Piatetsky-Shapiro, Padhraic Smyth, et al. 1996. “Knowledge Discovery and Data Mining: Towards a Unifying Framework.” In KDD, 96:82–88.\n\n\nGéron, Aurélien. 2017. Hands-On Machine Learning with Scikit-Learn and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems. 1 edition. Beijing ; Boston: O’Reilly Media.\n\n\nHayashi, Chikio, Keiji Yajima, Hans H. Bock, Noboru Ohsumi, Yutaka Tanaka, and Yasumasa Baba, eds. 1998. Data Science, Classification, and Related Methods: Proceedings of the Fifth Conference of the International Federation of Classification Societies (IFCS-96), Kobe, Japan, March 27, 1996. Studies in Classification, Data Analysis, and Knowledge Organization. Springer Japan. https://doi.org/10.1007/978-4-431-65950-1.\n\n\nMason, Hilary, and Christopher Wiggins. 2010. “A Taxonomy of Data Science.” Dataists.\n\n\nOjeda, Tony, Sean Patrick Murphy, Benjamin Bengfort, and Abhijit Dasgupta. 2014. Practical Data Science Cookbook. Packt Publishing Ltd.\n\n\nPorter, Michael D. 2020. “A Framework for Data Science.”\n\n\nRodriguez, Robert. 2012. “Big Data and Better Data.” Amstat News.\n\n\nTukey, John W. 1962. “The Future of Data Analysis.” The Annals of Mathematical Statistics 33 (1): 1–67.\n\n\nWirth, Rüdiger, and Jochen Hipp. 1999. “CRISP-DM: Towards a Standard Process Model for Data Mining.”"
  },
  {
    "objectID": "BSDS-Course-Info.html",
    "href": "BSDS-Course-Info.html",
    "title": "21  BSDS-Course-Info",
    "section": "",
    "text": "DS 2006\nDS 3005 Mathematics for Data Science"
  }
]