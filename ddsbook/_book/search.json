[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Defining Data Science",
    "section": "",
    "text": "Preface"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Defining Data Science - 4 + 1 Overview",
    "section": "",
    "text": "2 The 4 + 1 Model of Data Science\nR.C. Alvarado, UVA School of Data Science\nData Science is a complex and evolving field, but most agree that it can be defined as a combination of expertise drawn from three broad areas — computer science and technology, math and statistics, and domain knowledge — with the purpose of extracting knowledge and value from data. Many also associate it with a series of practical activities ranging from the cleaning and “wrangling” of data, to its analysis and use to infer models, to the visual and rhetorical representation of results to stakeholders and decision-makers.\nThis essay proposes a model of data science that is intended to go beyond the laundry-list definitions that dominate the discourse in the field today. Although these are not inaccurate, they do not get at the specific nature of data science or help distinguish it from adjacent fields such as computer science and statistics — fields whose members sometimes claim to already be doing data science. Without a clear understanding of its specific and unique nature, the field is subject to counterproductive turf battles in the academy as well as confusion in the workplace.\nWe define data science in terms of a multi-part model that represents core areas of expertise in the field and how they are related to each other. These are the areas of value, design, systems, and analytics. A fifth area, practice, integrates the other four in specific contexts. Together, these areas belong to every data science project, even if they are often unconnected and siloed in the academy.\nUnlike traditional academic disciplines, each area of the proposed model is inherently interdisciplinary, bringing together diverse and sometimes contrary perspectives under a common heading. The inherently interdisciplinary and pluralist nature of these areas is a distinctive feature of data science and a key differentiator between it and traditional disciplines.\nThe following describes how this model is derived and provides clues about how to interpret and apply the model to your own situation."
  },
  {
    "objectID": "intro.html#a-common-theme-the-image-of-the-pipeline",
    "href": "intro.html#a-common-theme-the-image-of-the-pipeline",
    "title": "1  Defining Data Science - 4 + 1 Overview",
    "section": "2.1 A Common Theme: The Image of the Pipeline",
    "text": "2.1 A Common Theme: The Image of the Pipeline\nA review of the literature on data science definitions, including sources from the adjacent fields data analysis and data mining, reveals that most invoke the image of a data processing pipeline — a sequence of events through which data flows as it moves from the consumption of so-called raw data to production of useful results. In this view, consumed data may come from a variety of sources, such as databases or intentional experiments or sensors, and results may be equally various, from the visual communication of analytical results to stake-holders to the development of an interactive data product for use on the web.1\nFor analytic purposes, it is helpful to view the pipeline image as a kind of story, a micronarrative that encodes and socializes an understanding about the practice of data science. All forms of work involve the sharing of stories in one form or another, both formally in the context of education and informally through the countless acts of training, mentorship, and imitation that go on everyday in the academy and the workplace. When we want to explain to a trainee, a peer, or a supervisor how something is done, we often resort to a story that is general enough to apply to a variety of contexts yet specific enough to be translatable into action. The data processing pipeline is like that, and the essays in which it appears are part of the ongoing discourse by which a community of practice grows and learns.\nBy thinking of the pipeline as a story we can analyze it with techniques developed by sociolinguists, folklorists, and students of literature to study how stories are structured and how they function socially and cognitively. One idea from this field is that stories can be broken down into elementary units, or “event functions,” and that these functions tend to have an invariant order within specific traditions (Propp and Dundes 1977). In addition, a given sequence of functions may exhibit overall patterns, such as a chiasmus structure, in which the functions at the beginning of a story mirror those a the end (Lévi-Strauss 1955). The following analysis applies these ideas."
  },
  {
    "objectID": "intro.html#the-primary-standard-sequence",
    "href": "intro.html#the-primary-standard-sequence",
    "title": "1  Defining Data Science - 4 + 1 Overview",
    "section": "2.2 The Primary Standard Sequence",
    "text": "2.2 The Primary Standard Sequence\nAn analysis of a representative sample of essays that define a data processing pipeline shows that the various pipeline stories consist of elements drawn from a primary standard sequence of about twelve elements, give or take a few depending on how one might expand or contract terms. These are listed and defined below by a core set of event types, with the understanding that many synonyms are employed in the examples.\n\nUnderstand. The work of developing a question that the acquisition and analysis of data can answer. Questions may range from the status of a scientific hypothesis the merit of a business value proposition. More broadly, this is the stage of framing and scoping the work that follows.\nPlan. The work of designing an experiment to produce data or of devising a strategy to acquire existing data relevant to the goals specified by the Understand phase.\nCollect. The work of producing or acquiring data. This covers a broad range of activities from taking surveys to generating signal data to harvesting data from databases to scraping websites.\nStore. The work of moving data from its collected state to a persistent store to house the data. This may be a file system or a database.\nClean. The work of fixing problems with data as collected and stored. This covers a wide range of activities, including the normalizing the data formats (such as dates) to the handling of missing data to the transformation of tables into some level of normal form.\nExplore. The work of investigating cleaned data for general patterns, distributions, simple clusters, outliers, sanity, etc. Exploratory data analysis provides a useful set of tools and perspectives for this work. In some models, this activity is expanded into more sophisticated forms of pattern discovery and becomes associated more directly with the Model phase. Visualization using tools such as box plots, scatter plots, histograms, etc. play a large role here.\nPrepare. The work of transforming data into a form suitable for the particular methods planned in the Model phase. This may mean generating tidy tables for statistical analysis, wide-format data for matrix operations, one-hot encoded feature spaces for input into deep learning systems, or graph structured data for network analysis. Also included here is feature engineering and dimensionality reduction.\nModel. The work of using data to train models or estimate parameters for the tasks of prediction, inference, etc. Internally, this phase comprises a sub-pipeline that operates closely with the previous and following steps and involves model selection, parameter tuning, validation, and testing. Additionally, this area is characterized by general approach, which may be based in classification (predictive modeling), inference, or simulation.\nInterpret. The work of making sense of the results of modeling the data, in terms of the terms set out by the Understand phase. This may mean making claims about causality, assigning real-world correlates to discovered patterns, or assessing the generalizability of a classifier.\nCommunicate. The work of representing data, model, and results to stakeholders as defined or implied by the Understand phase. The includes the work of visualization broadly conceived, from the creation on interactive visualizations to static infographics. It also includes story-telling and essay writing for both specialized and generalized audiences.\nDeploy. If the work results in a data product, such as real-time classifier to support a business process, this means standing up and maintaining the software product in a performant and scalable hardware environment.\nReflect. Between iterations of the pipeline, this is the work of reflecting on the strategic or philosophical significance of the work conducted. It also includes the ethical dimension of the work conducted.\n\nNote that the descriptions are not meant to be prescriptive; they are attempts at summarizing the descriptions found in the literature. With this composite view in place, it will be easier to address limitations and fill gaps in our effort to create an authentic definition of data science.\nNo one definition includes them all, but some are more comprehensive than others, and different disciplines emphasize different parts.\nFor example, Hayashi’s statistically-oriented definition of data science includes just three phases — design \\([1]\\), collect \\([3]\\), and analyze \\([8, 9]\\) — with an emphasis on the experimental design phase in which data are actually produced through thoughtfully designed experiments (Hayashi 1998).\nMason and Wiggins propose five — obtain \\([3, 4]\\), scrub \\([5]\\), explore \\([6]\\), model \\([8]\\), and interpret \\([9]\\) — which highlights two conditions that define industrial data science, the simultaneous availability of data (one obtains it, say through web scraping) and their poor condition relative to analysis, i.e. the need to scrub and wrangle them into usable form (Mason and Wiggins 2010).\nThe CRISP-DM model is the most comprehensive, with seven phases defined (if we include the unnamed but visually depicted function of storage), emphasizing the importance of understanding both the business proposition and data before anything is done with it (Wirth and Hipp 1999). It also modifies the metaphor of the pipeline, representing it as a circular and iterative process. However, unlike Donoho’s similarly comprehensive sequence (implied by the ordering of his six divisions of “greater data science”), it does not include a “meta” phase devoted to reflecting on the process as a whole (Donoho 2017).\nAs the CRISP-DM model shows, in some cases the pipeline is described as a circular process, where the output of one cycle serves as input to the next. Nevertheless, in these cases the sequential order of processing stages is maintained. Moreover, the image of the circle makes explicit what is implicit in the non-circular models, namely that the pipeline is never a one-off operation but repeated many times in the life-cycle of a project. What the circular models uniquely introduce is the idea that data processing is often a cybernetic process, a feedback loop where the results of one cycle inform the next. However, it should be noted that the idea of feedback foregrounded by circular models is not well developed. We shall see that the relationship between the endpoints is an area deserving of further research and theoretical development."
  },
  {
    "objectID": "intro.html#seven-chapters",
    "href": "intro.html#seven-chapters",
    "title": "1  Defining Data Science - 4 + 1 Overview",
    "section": "2.3 Seven Chapters",
    "text": "2.3 Seven Chapters\nA close look at the twelve phases listed above shows that a given phase may be more closely related to some phases than to others — in other words, the phases can be grouped thematically. For example, it seems clear that the Understand and Plan phases go together, just as Clean, Explore, and Prepare do. They belong together because we can imagine performing their associated activities together, and separately from the other phases. Also, in some cases the order of the phases within the group may change — for example, explore might precede cleaning in the event, as the two are often performed simultaneously — while the sequential order of the groups is less likely to vary. Finally, we can imagine assigning different teams to perform the labor in each thematic grouping, by virtue of the expertise required to carry them out.\nGiven this, the twelve-part composite pipeline can be reduced to seven groups:\n    \\(A\\) understand and plan\n    \\(B\\) collect and store\n    \\(C\\) clean, explore, and prepare\n    \\(D\\) model and interpret\n    \\(E\\) communicate\n    \\(F\\) deploy\n    \\(G\\) reflect\nEach of these thematic groups may be considered a “chapter” in the story. Note that the number of verbs in each chapter title does not necessarily predict the length of its content. For example, the chapter on “model and interpret” covers a wide range of activities from a variety of perspectives, including classical statistics, machine learning, and computational simulation. It’s a big and complicated chapter with its own pipeline, but it is just one chapter among seven."
  },
  {
    "objectID": "intro.html#an-arc-with-four-zones",
    "href": "intro.html#an-arc-with-four-zones",
    "title": "1  Defining Data Science - 4 + 1 Overview",
    "section": "2.4 An Arc with Four Zones",
    "text": "2.4 An Arc with Four Zones\n\n\n\nThe Standard Sequence as a Narrative Arc\n\n\nTo be sure, the middle chapter plays a central role in our story. If we think of the story as following a classical “there and back again” structure — a chiasmus pattern like \\(X_1, Y_1, Z, Y_2, X_2\\) — then chapter \\(D\\) is the pivot, while chapters \\(A,\\) \\(B,\\) and \\(C\\) mirror \\(E,\\) \\(F,\\) and \\(G\\). Thinking of the story in this way allows us to identify a parallel structure in the pipeline, connecting phases that are usually seen as separate. Specifically, we may visualize the pipeline as an arc, a U-shaped process in which chapters in the first half of the pipeline mirror the those of the second half. We may then group chapters by the pairs formed in this way, yielding four zones — \\(A\\) and \\(G\\) belong to zone \\(I\\), \\(B\\) and \\(F\\) to \\(II\\), \\(C\\) and \\(E\\) to \\(III\\), and \\(D\\) to \\(IV\\) — as in the following diagram:\n\n\n\nThe Arc Transposed\n\n\nWith this visualization, we can discern some interesting properties about the data science pipeline that are not obvious in the original sequential image. For one, the arc structure suggests that the two ends of the pipe are not separate; both make direct contact with the external world (relative to the internal world of the pipeline considered as a system). The external world — natural or social — from which data are pulled is the same world into which data products are inserted. This insight echoes the CRISP-DM model, which connects \\(A\\) and \\(G\\) (actually \\(F\\)), except that the two ends of the arc model are not directly connected. Instead, they come into contact with — and are separated by — the world in all of its complexity and unpredictability. The relationship between the effects caused by our data products \\(G\\) and the data we pull from the world \\(A\\) is not given but a matter of discovery — and often surprise. This is an important distinction between the model being proposed here and the simple idea that the pipeline forms a closed circle."
  },
  {
    "objectID": "intro.html#interpreting-the-themes",
    "href": "intro.html#interpreting-the-themes",
    "title": "1  Defining Data Science - 4 + 1 Overview",
    "section": "2.5 Interpreting the Themes",
    "text": "2.5 Interpreting the Themes\nAt this point, we can explore the unifying themes associated with the four zones in our arc model by transposing the preceding visualization, which draws attention to what is common to each pairing. This generates four candidate areas of data science expertise — activities that, although they appear on opposite ends of the pipeline, nevertheless share basic knowledge, know-how, and areas of concern.\nZone \\(IV\\) is the easiest to interpret in this way because as the pivot of the arc it is not paired. It represents the work of modeling a problem mathematically, as well as evaluating and interpreting the results of mathematical modeling. This work requires data to be available in a particular form — clean and organized, usually as “tidy” analytical tables — and it produces results that need to be prepared for consumption by non-specialists. As stated above, it is the pivot of the arc, the core process, where the prior phases constitute pre-processing and following phases post-processing.\nZone \\(I\\) is also relatively easy to interpret: the functions in this group each involve understanding the relationship between the pipeline and the external world, the messy interface between the enterprise of data science and the variety of real world situations in which it operates and impacts. Again, this relationship has a dual character, based on the difference between input and output, reading from and writing to the world. Note also that there is an open-ended quality to the this relationship that is elided by circular representations — there is always a knowledge gap between one’s understanding of a data product and its real effects on the world, effects which may in turn influence source data.\nWe may note in passing that \\(I\\) and \\(IV\\) can be contrasted in several ways — external vs internal, messy vs clean, exoteric vs esoteric, qualitative vs quantitative, existential vs essential, concrete vs abstract, periphery vs center, etc.\nWhen it comes to zones \\(II\\) and \\(III\\), the interpretation of results is less straightforward. This is because the reality of the kind of work performed in these areas is not as clear-cut as it is for \\(I\\) and \\(IV\\). Both \\(II\\) and \\(III\\) exhibit an internal complexity not found in the others, and the two are less clearly separable from each other than they are from the other two. One reason for this complexity is that here pure and applied forms of knowledge intermingle in ways that defy easy description from an academic perspective.\nFor example, the work of “data wrangling,” often considered essential to data science, spans the two domains and involves a complex mixture of specific technological know-how and general scientific principles. It turns out that the relationship between these kinds of knowledge is highly contested, as evidenced by the reception of Donoho’s “50 Years of Data Science,” which has been criticized for separating science from engineering and demoting the importance of the latter (Donoho 2017). Regardless of the validity of this criticism, there is without doubt a long-standing conflict between computational data mining and statistical data analysis over what counts as valid forms of knowledge, and this conflict emerges in the representation of zones \\(II\\) and \\(III\\) we find in our corpus.\nWe can take the conflict of interpretations over the status of technical knowledge in data science as a clue and use it to identify two broad dimensions that cross-cut the functions in zones \\(II\\) and $III$: technical know-how and abstract representation. We can reassign our labels to these dimensions, using the prime notation of \\(II'\\) and \\(III'\\) to indicate that they are transformation of our original themes.\nTechnical know-how \\(II´\\) involves expertise in developing and deploying software and hardware designed to handle data at scale, including high-performance computing, big data architectures (such as Hadoop and its descendants), and data-oriented programming languages and libraries. The topics associated with \\(II'\\) are highly specific and change rapidly relative to other forms of knowledge, and so are often omitted from, or under-represented in, academic curricula, even though to many they are the sine qua non of data science.\nAbstract representation \\(III´\\), on the other hand, involves expertise in areas ranging from how data are to be modeled for capture and analysis to how the results of analyses are to be presented to non-expert decision-makers. These areas of knowledge strive for formal generality over the long run; they are often expressed as grammars or design languages, frequently with visual modes (such as entity-relationship models and unified modeling language UML). They also include other forms of visualization, such as the plots developed for exploratory data analysis, such as box plots, and those used to represent statistical facts and analytical results in dashboards and infographics."
  },
  {
    "objectID": "intro.html#the-four-areas-plus-one",
    "href": "intro.html#the-four-areas-plus-one",
    "title": "1  Defining Data Science - 4 + 1 Overview",
    "section": "2.6 The Four Areas, Plus One",
    "text": "2.6 The Four Areas, Plus One\nWe are now ready to define and name the areas of data science expertise that emerge from an analysis of the pipeline considered as an arc. In each case, we want to identify the common context shared by the paired activities in each zone as well as the tension that exists between them by virtue of their occupying opposite sides of the pipeline. In many cases, although we can identify a shared theme in each zone’s work, the reality is that practitioners do not always interact or share disciplinary homes. One of the benefits of this model will be to identify these points of synergy and to identify new disciplinary boundaries.\nArea I: Value\nThe area of value is defined by the relationship of data science to the world from which it draws data and into which it inserts data products. More broadly, it concerns the primary motivations of data science — why do we practice data science in the first place? It combines the traditional discipline of ethics with the professional activities of business planning, policy making, developing motivations for scientific research, and other activities that have a direct impact on people and the planet. This is the area where we determine what we do versus what we do not do, in order to maximize societal and environmental benefit and minimize harm. It is also the area that looks inward to the other data science areas and provides guidance on such issues as algorithmic bias or open science. Common activities include the forming of value propositions that initiate data science projects, research into how data is created and used “in the wild,” understanding the ethics of data acquisition, manipulation, communication, and sharing, and the application of data products in the world.\nArea II´: Design\nThe area of design is defined by the relationship between human and machine forms of representation. This relationship is bidirectional: human-generated data flowing into the pipeline must be represented for machine consumption (H2M, or \\(H \\rightarrow M\\)), while analytically transformed data going out must be represented for human consumption (M2H, or \\(M \\rightarrow H\\)). This area therefore includes expertise in human-machine interaction as it appears at the points of both consuming data and producing data products. Activities here include the representation and communication of captured data for the work of analytics, e.g. in database modeling, the curation of data, and of complex data and analytical results to humans to drive decision-making and influence behavior. It also includes the making of things, with purpose (i.e. to solve problems) and intent (meaning, concision, focus). A key part of the area is the broad practice of what is often called visualization, the translation of complex quantitative information into visual (and other sensory) forms that non-experts can understand. In slightly more technical terms, the area of design focuses on what Zuboff called “informating,” the process by which the world is represented for computation and analytics, and also by which analytical models and results are represented to the world (Zuboff 1995). These two processes often produce competing representations — a private one of the world for the data scientist, and a public one for the world of the results of analytics. One task of this area is to reconcile these two representations.\nArea III´: Systems\nThe area of systems is defined by the technological infrastructure that is common to the pipeline but concentrated in the activities of wrangling data, deploying data products, and building out systems to support these activities at scale. This area includes expertise in infrastructure systems and architectures to support working with big data — big in terms of volume, velocity, and variety — and building high performance systems in both development and production environments. It includes the broad areas of hardware and software as such — computer technology as opposed to computer science. Key activities include developing cloud resources, building performant pipelines to ingest and aggregate data, developing networks of resilient distributed data, and writing and using software to accomplish tasks. This area is often referred to as “data engineering” or “machine learning engineering,” which, according to Owen, “is most of what Data Science is and Statistics is not” (Owen 2015).\nArea IV: Analytics\nThe area of analytics is defined by the practice of mathematical modeling based on data. This area includes what many consider to be the essence of data science, the combination of statistical methods with machine learning, along with information theory, optimization, network analysis, complexity theory, simulations, and other rigorous quantitative methods from a variety of fields. Although unified by a broad commitment to advanced mathematical models and computational algorithms, in reality this is a heterogeneous collection of competing schools and methods. Tensions include inference vs prediction, parametric vs non-parametric (kernel-based) methods, frequentist vs Bayesian statistics, analytic vs algorithmic solutions (including simulations), etc. Key activities include clustering, pattern recognition, regression, rule mining, feature engineering, model selection, performance evaluation, and a host of other activities. Although currently dominated by statistical methods, this area also includes the rule-based methods that dominated the field of artificial intelligence before the more recent successes of statistical learning and deep learning.\nArea V: Practice\nThe preceding four areas each represent areas of foundational knowledge, forms of expertise that can be taught as more or less separate subjects. In practice, however, these areas represent the interlocking parts of a division of labor that are integrated in the pipeline. This area consists of actual activities that brings people together to combine expertise from each of the four areas. It is characterized by data science teams working together and with external parties to develop solutions and projects that are responsible, authentic, efficient, and effective. Practice is also where the core areas of data science come into contact with a broad spectrum of domain knowledge and real world problems. The following diagram shows the central, integrative role played by practice:\n\n\n\nThe Integrative Role of Practice"
  },
  {
    "objectID": "intro.html#two-principal-components",
    "href": "intro.html#two-principal-components",
    "title": "1  Defining Data Science - 4 + 1 Overview",
    "section": "2.7 Two Principal Components",
    "text": "2.7 Two Principal Components\nIs there a way to understand how the four primary areas are related to each other, beyond their being composed of functions from the same pipeline? Put another way, does the pipeline-as-arc model exhibit any structural features that will help us conceptualize the broader space of data science? Two such features stand out: (1) the opposition between concrete and abstract forms of representation, and (2) between human and machine processing.\nRegarding the concrete and the abstract, it’s clear that the arc model has a metric quality to it: as one moves toward the pivot point of analysis, one moves away from the concrete messiness of reality as experienced to the “tidy” and abstract world of mathematics; similarly, as one moves from the pivot back to the world, there is a requirement to convert esoteric results into more humanly intelligible forms, often through a process of concretization; visualizations succeed by employing concrete metaphors that flesh out mathematical ideas that are notoriously detached from the imagination — no one can imagine, for example, n-dimensional spaces beyond a handful of dimensions. The arc describes a dialectic of abstraction and concretization that defines the ebb and flow and data science work.\n\n\n\n\n \nConcrete\nAbstract\n\n\n\n\nHuman\nIVALUE\nII’DESIGN\n\n\nMachine\nIII’SYSTEMS\nIVANALYTICS\n\n\n\n\nThe Four Areas in Two Dimensions\n\nThe dimension of human and machine processing exhibits a similar duality, that between the conversion of information from humanly accessible forms, such as given by data acquired by instruments, into machine readable and processible forms, and the reverse. The process of moving from human to machine representations is a large part of what data capture, modeling, and wrangling is all about, while the process of converting the results of machine learning, broadly conceived, into humanly actionable form is what visualization and productization are all about. The reality of this dualism is captured by the concept of human-computer interaction (HCI), an established field that is applicable to both sides of the arc.\nHow do the four fundamental areas map onto these two dimensions? We can define each area as a combination of one pole from each duality; the four areas result from all possible permutations of the two dimensions. This produces the following high level characterizations of each area: (1) Value is concerned with concrete humanity, (2) Design with abstract humanity, (3) Analytics with abstract machinery, and (4) Systems is concerned with concrete machinery. All of these make intuitive sense, with the exception of Design. This is consistent, however, with the fact that the area of Design emerges from this analysis as an undervalued and not well understood area of expertise, even though Yau emphasized it early on (Yau 2009b). Indeed, one of the consequences of this analysis is to train our attention on this area of knowledge and to develop it further.\nOne exciting interpretation of the two dimensions defined here is that they correspond to two principal components that undergird the general field of data science. As components, these axes define two orthogonal dimensions within which all the specific topics of data science may, in principle, be plotted. The reality behind these axes may be that they represent cognitive styles associated with the division of labor implied by the data science pipeline.\nPC1: Human versus Machine\nThe human-machine axis accounts for the most variance in the field. This seems evident from the fact that Conway’s Venn diagram model of data science represents only the machine side of our model, with practice replaced by “substantive expertise” (Conway 2010). The human side — Value and Design — is left out, or short-changed by being lumped in with domain knowledge. The very fact that the human side has to be explained and added to the model suggests strongly that it defines a pole at some distance from the areas of knowledge described in Conway’s model. The human pole refers to humanity understood as situated in their historical, social, and cultural milieu. It is synonymous with human experience. The machine pole refers to the technoscientific apparatus of formal, quantitative reasoning that operates on representations of the human and the world. In the context of data science, it is more or less synonymous with machine intelligence, broadly conceived to include machine learning but also other modes of analysis on the spectrum of prediction and inference. Given these poles, the human-machine axis represents the opposition between humanistic disciplines that seek to understand human experience as such, and the formal sciences that employ machine intelligence, broadly conceived, to interpret that experience as represented and aggregated in the form of data.\nPC2: Concrete versus Abstract\nThe abstract-concrete axis accounts for the difference between two forms of knowledge, roughly between direct experience and the indirect representation of that experience enabled through data. Both the realm of Value and Systems involve immersion in the messy details of lived experience — and direct acquaintance with the devils in those details. This is the messy world of hacks and ironies. The realms of Design and Analysis, on the other hand, are founded on abstract representations that strive for clear and distinct purity, and which allow for deductive reasoning to succeed at the cost of simplifying assumptions and reduced representations. This is the orderly world of models. The concrete pole refers to situated knowledge, knowledge as understood by hackers and makers, but also ethnographers who seek to maximize thick description in their work. It represents concrete materiality. The abstract pole refers to formal knowledge, knowledge in the form of mathematical symbolism, deductive proofs, and algorithmic patterns. It is abstract form. Given these poles, the concrete-abstract axis is roughly the opposition between applied and pure forms of knowledge, between those that embrace materiality and those that seek purity of form."
  },
  {
    "objectID": "intro.html#final-representation",
    "href": "intro.html#final-representation",
    "title": "1  Defining Data Science - 4 + 1 Overview",
    "section": "2.8 Final Representation",
    "text": "2.8 Final Representation\nThe result of the preceding may be represented by the following graphic.\n\n\n\nThe 4+1 Model of Data Science\n\n\nThis visualization represents data science as composed of specific and complementary forms of knowledge. The vertical axis defines the dominant polarity between analysis — the how of data science, often identified entirely with it, contrasted with the why of data science, from which data science derives its meaning and value as a profession. The horizontal access defines the polarity of methods that are often obscured in academic definitions of data science — the supporting practices that make the Analytics component work in the first place."
  },
  {
    "objectID": "intro.html#concluding-remarks",
    "href": "intro.html#concluding-remarks",
    "title": "1  Defining Data Science - 4 + 1 Overview",
    "section": "2.9 Concluding Remarks",
    "text": "2.9 Concluding Remarks\nThe point of the 4 + 1 model, abstract as it is, is to provide a practical template for strategically planning the various elements of a school of data science. To serve as an effective template, a model must be general. But generality if often purchased at the cost of intuitive understanding. The following caveats may help make sense of the model when considering its usefulness when applied to various concrete activities.\nThe model describes areas of academic expertise, not objective reality. It is a map of a division of labor writ large. Although each of the areas has clear connections to the others, the question to ask when deciding where an activity belongs is: who would be an expert at doing it? The realms help refine this question: the analytics area, for example, contains people who are good at working with abstract machinery. The four areas have the virtue of isolating intuitively correct communities of expertise. For example, people who are great at data product design may not know the esoteric depths of machine learning, and that adepts at machine learning are not usually experts in understanding human society and normative culture.\nEach area in the model contains a collection of subfields that need to be teased out. Some areas will have more subfields than others. Although some areas may be smaller than others in terms of number of experts (faculty) and courses, each area has a major impact on the overall practice of data science and the quality of an academic program’s activities. In addition, these subfields are in an important sense “more real” than the categories. We can imagine them forming a dense network in which the areas define communities with centroids, and which are more interconnected than the clean-cut image of the model implies.\nThe principal components abstract/concrete and human/machine are meant to help imagine the kinds of activities that belong in each area, through their connotations when combined to form the four bigrams — concrete human, abstract human, concrete machine, and abstract machine. For example, the area of value as the realm of the “concrete human” (or perhaps “concrete humanity”) is meant to connote what the Spanish philosopher Unamuno called the world of “flesh and bone” within which we live and die, that is, where things matter. On the other hand, analytics as the realm of the “abstract machine” is meant to connote the platonic world of mathematical reasoning which, since Euclid, has been characterized by rigorous, abstract, deductive reasoning that has literally been described as an abstract machine (see Alan Turing).\nAt the center of this model and each area is people. Even in the area classified as “abstract machine,” people and human thinking is at the center."
  },
  {
    "objectID": "intro.html#references",
    "href": "intro.html#references",
    "title": "1  Defining Data Science - 4 + 1 Overview",
    "section": "2.10 References",
    "text": "2.10 References"
  },
  {
    "objectID": "design.html",
    "href": "design.html",
    "title": "Design",
    "section": "",
    "text": "This section will provide a overview of the the Design space through three different lenses. The School of Data Science’s perspective on the curriculum and skills that compose this area, the view from industry to included recorded lectures from experts that have participated in the Foundations of Data Science Course at UVA and through the point of view of researchers that are working and contributing to this area of the field.\nIt is important to note that all these areas are evolving and have overlapping content area, but Design in particular can be difficult to neatly fold into well-defined set of easily quantifiable skills."
  },
  {
    "objectID": "design-sds.html",
    "href": "design-sds.html",
    "title": "2  Design SDS",
    "section": "",
    "text": "Overview of the Curriculum and Skills that the School of Data Science and UVA believes to be included in the domain area of Design."
  },
  {
    "objectID": "design-lab.html",
    "href": "design-lab.html",
    "title": "3  Design Lab",
    "section": "",
    "text": "Overview of the Design lab for the current semester for Foundations of Data Science Course at UVA."
  },
  {
    "objectID": "design-external.html",
    "href": "design-external.html",
    "title": "4  Design External to the School",
    "section": "",
    "text": "Contains a overview of the perspective from industry or others outside of academia on the Design space of Data Science."
  },
  {
    "objectID": "design-case-study.html",
    "href": "design-case-study.html",
    "title": "5  Design Case Study",
    "section": "",
    "text": "Overview of the Design case study for the current semester for the Foundations of Data Science course."
  },
  {
    "objectID": "value-lab.html",
    "href": "value-lab.html",
    "title": "7  Labs Rubric - Guess Who",
    "section": "",
    "text": "DS 1001 - Spring 2023 - Professors Wright and Alonzi Due: End of lab period (or later that day) Submission format: Word doc or PDF summarizing your findings\nIndividual Assignment\nGeneral Descripition: This lab is design for you to reflect on the process used for winning the game Guess Who. Students should work in teams to play the game and take notes on the processes that work best for quickly guessing the opponents card. This will likely include the creation of a tree based decision diagram, a example will be provided in class.\nPreparatory Assignments - None\nWhy am I doing this? In order to allow you to reflect on how data is used to generated decision/predictive algorithms and what issues could arise as a result. Work to incorporate the materials you’ve been exposed to up to this point as you work through the lab.\nWhat am I going to do? You are working through a interactive process with your group to find the optimal list of yes/no questions for any given card. Play the game as many times as necessary to construct a list of the top five questions using a tree based diagram.\nAnswer these questions:\n\nIn this situation what is the dataset and what is the algorithm?\nHow did your approach to identifying cards change throughout the lab period?\nAre there cards that are easier to identify, why?\nThis is just a game, but given the materials you’ve been exposed to what concerns do you have about this process and algorithmic decision making?\n\nTips for success:\n\nDon’t worry about winning\nWork as a team with your group\nTry to document the process used right at the start of the lab, don’t wait till after you have played to start taking notes\n\nHow will I know I have succeeded\n\n\n\n\n\n\n\nSpecs Category\nSpecs Details\n\n\n\n\nFormatting\n\nSubmit Via Canvas\nText answers to questions\n\n\n\nText\n\nGoal: The questions are designed to be answer during or right after the lab period.\nUse a few sentences to answer each question in Canvas\n\n\n\n\nAcknowledgements: Special thanks for Jess Taggart from UVA CTE for coaching us. This structure is pulled directory from Steifer & Palmer (2020)."
  },
  {
    "objectID": "value-lab-II.html",
    "href": "value-lab-II.html",
    "title": "8  Labs Rubric - AI Fairness 360",
    "section": "",
    "text": "DS 1001 - Spring 2023 - Professors Wright and Alonzi Due: End of lab period (or later that day) Submission format: Word doc or PDF summarizing your findings\nIndividual Assignment\nGeneral Descripition: This lab is designed for you to get exposure to AI fairness approaches in a no code environment on the website AI Fairness 360. You will be able to work through the various fairness methods at different stages of the pipeline and reflect on which methods seem to work the best on the given datasets.\nPreparatory Assignments - None\nWhy am I doing this? In order to give you exposure to and practice with the various methods being developed and deployed in the ML fairness space. After completing the lab you’ll have a better sense of how these tools are used, when they are used and how the work.\nWhat am I going to do? The AI Fairness 360 website has a demo module that includes three datasets. Work through the demo on all three datasets, trying all the methods provided, and answer the questions below.\nAnswer these questions:\n\nFor each protected class variable which evaluation methods showed bias to be present?\nNote how each method preform at removing bias.\nWas the accuracy of the model effected when using the various approaches, if so how?\nGiven the above what are some patterns you noticed, which methods seem to work the best, where in the data process are these methods located (pre/in/post).\n\nTips for success:\n\nTake careful notes as you go through each method\nHave fun\n\nHow will I know I have succeeded:\n\n\n\n\n\n\n\nSpecs Category\nSpecs Details\n\n\n\n\nFormatting\n\nSubmit Via Canvas\nText answers to questions\n\n\n\nText\n\nGoal: The questions are designed to be answer during or right after the lab period.\nBullett points are fine for the first three questions, paragraph from for the fourth\n\n\n\n\nAcknowledgements: Special thanks for Jess Taggart from UVA CTE for coaching us. This structure is pulled directory from Steifer & Palmer (2020)."
  },
  {
    "objectID": "value-lab-III.html",
    "href": "value-lab-III.html",
    "title": "9  Labs Rubric - Explainable AI Interview",
    "section": "",
    "text": "DS 1001 - Spring 2023 - Professors Wright and Alonzi Due: End of lab period (or later that day) Submission format: Word doc or PDF summarizing your findings\nIndividual Assignment\nGeneral Descripition: This lab is designed for you to verbalize the concept of Explainable AI to someone that is unfamiliar with the concepts and track their responses.\nPreparatory Assignments - None\nWhy am I doing this? This will allow you to work on communicating the concepts thus understanding them better and judging whether trust is increased with the knowledge that explainable approach to AI models are present.\nWhat am I going to do? Interview three people that have somewhat limit knowledge of Machine Learning and AI. Ask the first two questions then provide a brief overview of Explainable AI and then ask the remaining three questions. After all three interviews are complete provide a brief summary of what patterns you notice (a paragraph or two is fine)\nAnswer these questions:\n\nDo you believe that AI has a large influence over society currently?\nOn a scale from 1 to 10, with 10 being total trust and 1 being no trust: Do you believe that AI models can be trusted and used in a way that promotes positive outcomes for individuals and society?\nGive a brief overview of the Data Ethic concepts with a emphasis on Explainable AI models.\nGiven what you just heard does that in anyway change your answer to question 2, such that, if explainable AI models were more commonly used would you be more likely to believe that they can promote positive outcomes for individuals and society? (scale of 1 to 10 for you belief the AI models can promote positive outcomes)\nAre you interested in learning more about Explainable AI as a concept?\n\nTips for success:\n\nKeep your over of data Ethics and Explainable AI at a high level and use the slides provided in class as needed.\nFeel free to improvise and ask additional questions if you’d like or change the questions slightly.\n\nHow will I know I have succeeded:\n\n\n\n\n\n\n\nSpecs Category\nSpecs Details\n\n\n\n\nFormatting\n\nSubmit Via Canvas\nSummary of your three interviews in one document\nA reflection on any patterns you noticed in the answers\n\n\n\nText\n\nGoal: The interview is designed to be fairly quick, 10 minutes or so for each. All together the assignment should take approximately one hour.\n\n\n\n\nAcknowledgements: Special thanks for Jess Taggart from UVA CTE for coaching us. This structure is pulled directory from Steifer & Palmer (2020)."
  },
  {
    "objectID": "value-case-study.html",
    "href": "value-case-study.html",
    "title": "11  Value Case Study",
    "section": "",
    "text": "of this Value domain"
  },
  {
    "objectID": "analytics-sds.html",
    "href": "analytics-sds.html",
    "title": "16  Analytics UVA DS",
    "section": "",
    "text": "Code\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr=np.arange(0,2,.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'}\n)\nax.plot(theta,r)\nax.set_rticks([.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\nFigure 16.1: A line plot on a polar axis"
  },
  {
    "objectID": "analytics-lab.html",
    "href": "analytics-lab.html",
    "title": "Defining Data Science",
    "section": "",
    "text": "Labs Rubric - Mastermind - Entropy\nDS 1001 - Spring 2023 - Professors Wright and Alonzi\nDue: End of lab period (or later that day)\nSubmission format: Word doc or PDF summarizing your findings\nIndividual Assignment\nGeneral Descripition: This lab is designed to introduce the basic concepts behind information gain and entropy. Mastermind is a game were through a series of questions one player tries to determine a “code” created by another player. In doing so the value of asking questions that provide a high level of information will become paramount.\nPreparatory Assignments - None\nWhy am I doing this? In order to gain a better understanding of information gain and entropy through hands on experience.\nWhat am I going to do? Work with a partner or group of three and alternate playing the game. At the onset of each game the code-maker should calculate the entropy of the chosen pegs for the “code”. The code-breaker should note the results of the game . This should include whether the code was broken or not and the number of rounds used. Try to be the code-maker and breaker at least 3 times. The code-maker should intentionally use very different types of “code”, this should impact how the game is played.\n\nNote the range of entropy for binary cases is 0 to 1 for more than 2 classes it is 0 to log2 k, where k is the number of classes.\n\nAnswer these questions:\n\nWhat combinations of pegs (code) seemed to be harder to break?\nDid your approach to asking questions change as you played?\nDescribe where in the game information gain is being presented?\n\nTips for success:\n\nDon’t worry about winning, instead think about what is happening during game play.\nWork as a team with your group.\nTry to document the process used right at the start of the lab, don’t wait till after you have played to start taking notes.\n\nHow will I know I have succeeded:\n\n\n\n\n\n\n\nSpecs Category\nSpecs Detail\n\n\n\n\nSubmission\nSubmit via Canvas a PDF or Word Document\n\n\nText\n* Answer the above questions * When you were the code-maker submit the Entropy * When you were the code-breaker submit the results\n\n\n\nAcknowledgements: Special thanks for Jess Taggart from UVA CTE for coaching us. This structure is pulled directory from Steifer & Palmer (2020)."
  },
  {
    "objectID": "appendix-sources.html",
    "href": "appendix-sources.html",
    "title": "20  Primary Sources",
    "section": "",
    "text": "About the Sources\nThe primary sources on which the conclusions of this essay are based comprise a variety of documents, from technical journals to blog posts to internal reports. They come from a range of viewpoints, from data analysis and statistics to data mining and data science per se. For the purposes of the essay, we select a more or less representative subset across these axes of variation. With respect to representativeness, in some cases a document was chosen for its influence, in others, such as the post by Dataman, because it is considered more or less typical of a common genre.\nThe documents chosen are listed below in chronological order, beginning with Tukey’s seminal essay on data analysis and ending with contempory explainers. Included also are the definitions of the CRISP-DM and KDD processes which are the most developed pipeline models.\nEach source entry below contains a short description of the source and its context, and then a list of the phases cited by the authors as fundamental to data processing. These phases are also mapped onto the standard sequence described in the main part of this essay, listed here for convenience.\nMappings are indicated by an arrow pointing to the subset of terms from the standard sequence, e.g. … \\(\\rightarrow [Explore]\\) These mappings are also aggregated into a composite pipeline and displayed the table below; each model row is referenced by its key as defined in the entries.\nNote that in most cases these phases are explicitly described as a process and often as a pipeline. When they are not, the implication is strong. In some cases, the process is likened to a cycle, emphasizing the connection between the endpoints of the pipeline, which is also emphasized by the 4+1 model.\nA final feature added to each entry is a two-value indicator of bias — statistics and data mining. This is meant to capture the intellectual origin of the model, given that statistics and data mining define the poles of one of main axes of variance that defines the field of data science. This difference roughly corresponds to the “two cultures” described by Breiman Breiman (2001)."
  },
  {
    "objectID": "appendix-sources.html#list-of-sources",
    "href": "appendix-sources.html#list-of-sources",
    "title": "20  Primary Sources",
    "section": "20.1 List of Sources",
    "text": "20.1 List of Sources\n\n20.1.1 Tukey on Data Analysis\nKey: Tukey\nYear: 1962\nSource: Tukey (1962) URL\nBias: Statistics\nIn this classic essay, Tukey introduces the concept of data analysis, which he distinguishes from mathematical statistics and likens to an empirical science. He defines data analysis as an empirical process with phases including “… procedures for analyzing data, techniques for interpreting the results of such procedures, ways of planning the gathering of data to make its analysis easier, more precise or more accurate, and all the machinery and results of (mathematical) statistics which apply to analyzing data” (p. 2). Unpacking this statement yields a four phase model.\n\nPlanning: This phase includes “ways of planning the the gather of data to make its analysis easier.” \\(\\rightarrow [Plan]\\)\nGathering: The gathering of data, either through creation or by acquisition of “data already obtained” (p. 40). Includes also the shaping of data “to make its analysis easier,” which corresponds to our concept of Preparation. \\(\\rightarrow [Collect, Prepare]\\)\nAnalyzing: This is where data are analyzed with “all the machinery and results of (mathematical) statistics.” \\(\\rightarrow [Explore, Model]\\)\nInterpreting: “techniques for interpreting the results of” analysis. \\(\\rightarrow [Interpret]\\)\n\n\n\n20.1.2 Fayyad on KDD\nKey: KDD\nYear: 1996\nSource: Fayyad et al. (1996) URL→\nBias: Data Mining\n\nKDD, or Knowledge Discovery in Databases, emerged in the late 1980s as both datasets and the computational resources to work with them became abundant. These resources included commercial databases and personal computers. In many ways the most adjacent field to contemoporary data science, this approach is unabashedly dedicated to finding patterns in data prior to developing a probabilistic model to justify their use. Fayyad’s essay identifies five steps (Fayyad et al. 1996: 84). He emphasizes the highly iterative and cyclical nature of the process, arguing that it “may contain loops between any two steps.” Another significant aspect of this conception of the pipeline is the role of exploration in the analytical phase: “Data Mining is a step in the KDD process consisting of applying data analysis and discovery algorithms that, under acceptable computational efficiency limitations, produce a particular enumeration of patterns over the data ….” (p. 83)\n\nSelection: Creating a target data set, or focusing on a subset of variables or data samples, on which discovery is to be performed. \\(\\rightarrow [Collect]\\)\nPre-processing: Cleaning and pre processing the data in order to obtain consistent data. \\(\\rightarrow [Clean]\\)\nTransformation: Transformation of the data using dimensionality reduction and other methods. \\(\\rightarrow [Prepare]\\)\nData Mining: Searching for patterns of interest in a particular representational form, depending on the DM objective (usually, prediction). \\(\\rightarrow [Model]\\)\nInterpretation/Evaluation: Interpretation and evaluation of the mined patterns. \\(\\rightarrow [Interpret]\\)\n\n\n\n20.1.3 Azevedo on SEMMA\nKey: SEMMA\nYear: 1996\nSource: Azevedo and Santos (2008)\nBias: Statistics\nThe SEMMA model was developed the by SAS institute in 1996 as part of the documentation for their product, SAS Enterprise Miner. Even so, the model is referenced outside of this context, often as a comparison to KDD and CRISP-DM. Its bias towards statististics is evident in the first step.\n\nSample: Sampling the data by extracting a portion of a large data set big enough to contain the significant information, yet small enough to manipulate quickly. \\(\\rightarrow [Collect]\\)\nExplore: Exploration of the data by searching for unanticipated trends and anomalies in order to gain understanding and ideas \\(\\rightarrow [Explore]\\)\nModify: Modification of the data by creating, selecting, and transforming the variables to focus the model selection process \\(\\rightarrow [Prepare]\\)\nModel: Modeling the data by allowing the software to search automatically for a combination of data that reliably predicts a desired outcome. \\(\\rightarrow [Model]\\)\nAssess: Assessing the data by evaluating the usefulness and reliability of the findings from the DM process and estimate how well it performs. \\(\\rightarrow [Interpret]\\)\n\n\n\n20.1.4 Hayashi on Data Science\nKey: Hayashi\nYear: 1998\nSource: Hayashi et al. (1998) URL→\nBias: Statistics\nThe Japanese statistician Chikio Hayashi adopted the term “data science” in the early 1990s to define a field that did not succumb to what he saw to be the errors of both statistics and data analysis. He argued that mathematical statistics had become too attached to problems of inference and removed from reality, while data analysis had lost interest in understanding the meaning of the data it deals with. His definition of data science is decidely processual: “Data Science consists of three phases: design for data, collection of data and analysis on data. It is important that the three phases are treated with the concept of unification based on the fundamental philosophy of science …. In these phases the methods which are fitted for the object and are valid, must be studied with a good perspective.” (p. 41) Similar to KDD and CRISM-PM, Hayashi envisioned this process as a spiral, oscillating between poles if what he called “diversification” and “simplification.” Note also that each of these terms, as described, comprises more than on of the standard sequence phases.\n\nDesign: Surveys and experiments are developed to capture data from “multifarious phenomena.” \\(\\rightarrow [Understand, Plan]\\)\nCollection: Phenomena are expressed as multidimensional or time-series data; properties of the data are made clear. At this stage, data are too complicated to draw clear conclusions. (Representation) \\(\\rightarrow [Collect, Explore, Prepare]\\)\nAnalysis: By methods of classification, multidimensional data analysis, and statistics, data structure is revealed. Simplification and conceptualization. Also yields understanding of deviations of the model, which begins the cycle anew. (Revelation) \\(\\rightarrow [Model, Interpet]\\)\n\n\n\n20.1.5 Wirth and Hipp on CRISP-DM\nKey: CRISPDM\nYear: 1999\nSource: Wirth and Hipp (1999) URL→\nBias: Data Mining\nBy the late 1990s, the practice of data mining had become widespread in industry and globally. In 1999 the Cross Industry Standard Process for Data Mining (CRISP-DM) was developed in Europe as a comprehensive and general model to support the use of data mining in a broad range of sectors in a principled manner. Designed to work within a project management framework, this model is by far the most developed, and it continues to influence the field of data science to this day. Like KDD before it, the model emphasizes the cyclic and recursive nature of the process, and this perspective is reflected in the circular diagram that often accompanies its presentation. The steps below are based on the summary presented in Wirth and Hipp’s essay.\n\n\n\nProcess diagram showing the relationship between the different phases of CRISP-DM (Wikipedia)\n\n\n\nBusiness Understanding: Understanding project objectives and requirements from a business perspective. Includes the development of a plan. \\(\\rightarrow [Understand, Plan]\\)\nData Understanding: The initial data collection and activities to get familiar with the data, e.g. to identify data quality problems, to discover first insights into the data, or to detect interesting subsets to form hypotheses for hidden information. This is really two phases — Collection and Exploration — which are combined because of their close, iterative relationship. \\(\\rightarrow [Collect, Explore]\\)\nData Preparation: Construction of the final dataset for analytical use. Tasks include table, record, an attribute selection, data cleaning, construction of new attributes, and transformation of data for modeling tools. \\(\\rightarrow [Clean, Prepare]\\)\nModeling: Modeling techniques are selected and applied, parameters calibrated. Modeling techniques include a broad range of unsupervised and supervised methods. As with KDD, there is an emphasis on pattern discovery, which has the effect of promoted methods that other models place squarely in the Explore phase of the standard sequence. \\(\\rightarrow [Model]\\)\nEvaluation: Evaluation of model performance by both intrinsic and extrinsic measures. Regarding the latter, a key objective is to determine if an important business issue has not been sufficiently considered. \\(\\rightarrow [Interpret]\\)\nDeployment: The knowledge gained by the model is presented in a way that the customer can use it. This may be something as simple as a report or as complex as a repeatable data mining process. In many cases the user, not the data analyst, will carry out the deployment. \\(\\rightarrow [Deploy]\\)\n\n\n\n20.1.6 Mason and Wiggins on OSEMI\nKey: OSEMI\nYear: 2010\nSource: Mason and Wiggins (2010) URL→\nBias: Data Mining\nAfter the phrase “data science” went viral (circa 2009), there were many efforts to make sense of the idea. In 2010 Drew Conway posted his Venn diagram of data science (Conway 2010). The same year, another influential model, based explicitly on the pipeline, came from Mason and Wiggins in a blog post hosted at O’Reilly’s Tech Radar site. In contrast to previous models rooted in statistics, this model assumes that data are abundant and available, such as data scrapable from the Web.\n\nObtain: Gather data from relevant sources through APIs, web scraping, etc. \\(\\rightarrow [Collect]\\)\nScrub: Clean data and convert data to machine readable formats. Clearning includes handling missing data, inconsistent labels, or awkward formatting; stripping extraneous characters; normalizing values, etc. \\(\\rightarrow [Clean, Prepare]\\)\nExplore: Find significant patterns and trends using statistical and data analytic methods, such as visualizing, clustering. Also includes transformations of the for more effective analysis, such as dimensionality reduction. \\(\\rightarrow [Explore]\\)\nModel: Construct methods to predict and forecast. These methods include those of inferential statistics and predictive machine learning. \\(\\rightarrow [Model]\\)\nInterpret: Making sense of the results as well as evaluating the performance of models. May involve domain experts. Also includes methods such as regularization that make models interpretable to those who use them, e.g. scientists or business people. \\(\\rightarrow [Interpret]\\)\n\n\n\n20.1.7 Ojeda, et al. on Data Science\nKey: Ojeda+\nYear: 2014\nSource: Ojeda et al. (2014) URL→\nBias: Data Mining\nBy 2014, data science had become a widespread practice in industry and the academic, and explanations of its nature became the subject of many books. This text is one of a genre that presents the field as a process, perhaps due to the influence of the CRISP-DM and OSEMI models, and uses the expression pipeline throughout. Note that the model defined in this book is not presented here as canonical. It suffers from various inconsistences, such as the labeling of steps in the text representation of the pipeline versus those on diagrams. It is included to demonstrate the pervasiveness of the model.\n\nAcquisition: Acquire the data from relational databases, NoSQL and document stores, web scraping, distributed databases (e.g. HDFS on a Hadoop platform), RESTful APIs, flat files, etc. Consistent with the other data mining models, the emphasis here is on working with available data, not generating it. \\(\\rightarrow [Collect]\\)\nExploration and understanding: Understand the data and how it was collected or produced; this often requires significant exploration. Note that this step does not correspond to exploration in the sense of exploratory data analysis (EDA). Rather, it reflects the position of the data scientist as the receiver of someone else’s data and the need to infer what would normally belong to the first step of the standard squence \\(Understand\\). \\(\\rightarrow [Understand]\\)\nMunging, wrangling, and manipulation: Convert the data into the form required for analysis. This includes a wide range of activities, such as those mentioned in previous models. However, it also conflates the standard phases \\(Clean\\) and \\(Prepare\\). \\(\\rightarrow [Clean, Prepare]\\)\nAnalysis and modeling: Apply statistical and machine learning methods, including clustering, categorization, and classification. One presumes that the standard step of \\(Explore\\) is included here. \\(\\rightarrow [Explore, Model]\\)\nCommunicating and operationalizing: At the end of the pipeline, we need to give the data back in a compelling form and structure, sometimes to ourselves to inform the next iteration, and sometimes to a completely different audience. The data products produced can be a simple one-off report or a scalable web product that will be used interactively by millions. \\(\\rightarrow [Communicate, Deploy]\\)\n\n\n\n20.1.8 Caffo, et al. on Data Science\nKey: Caffo+\nYear: 2015\nSource: Caffo, Peng, and Leek (2015) URL→\nBias: Statistics\nBy 2015, many universities had begun offering degrees in data science, typically at the masters’ level, with the intention of meeting the high demand for data scientists. Professors Caffo, Peng, and Leek’s book was written to accompany a course in Exectutive Data Science, offered by Johns Hopkins University through Coursera. Their model is relatively high level, consisting of five phases, given the target audience of those in charge of data science teams. As with other models, this model emphasizes the iterative nature of each phase, both internally and between phases. And as with many statistics-oriented conceptions of data science, this model emphasizes the Understand phase and skips over the technical issues of storing and modeling the data.\n\nQuestion.: Pose a research question and specify what is to be learned from data to answer it. The question determines the data to be obtained and the type of analysis to perform. Included determing the type of question, including descriptive, exploratory, inferential, causal, predictive, and mechanistic. An alternate approach here is hypothesis generation, which may be suitable when data already exist but a question is not well-developed. In this scenario, the data scientist may skip to the next step to determine the value of the data. Once a question is developed, then it may be necessary to acquire more data, and then go through the process. \\(\\rightarrow [Question, Collect]\\)\nExploratory data analysis: Explore the data to determine if the data are suitable for answering the question and if more data need to be collected. For example, determine if there are enough data and if it is missing key variables. In addition, develop a sketch of the solution. Include a freamework for challenging results and to develop robust evidence for answering your question.  \\(\\rightarrow [Explore]\\)\nFormal modeling: Identify the parameters to estimate based on the sketch.   \\(\\rightarrow [Model]\\)\nInterpretation: Determine if the modeling results align with the initial expections during the Question phase and before the acquisition of data. Consider the totality of the evidence developed after attempting to fit different models, weighing the different pieces of evidence.   \\(\\rightarrow [Interpret]\\)\nCommunication: Communicate findings to various audiences, either internal to the organization or external. Includes translating findings into action by virtue of effectively communicating results to decision-makers. \\(\\rightarrow [Communicate]\\)\n\n\n\n20.1.9 Donaho on Data Science\nKey: Donoho\nYear: 2017\nSource: Donoho (2017) URL→\nBias: Statistics\nAs data science became viral in the 2010s, academic statisticians frequently expressed concern that they were “disconnected from the new (and vaguely defined) community of data scientists, who are completely identified with Big Data in the eyes of the media and policymakers” (Rodriguez 2012). “Aren’t We Data Scientists?” asked Marie Davidian, then president of the American Statistical Association, in 2013 (Davidian 2013). In response to this growing sentiment, Donoho’s essay reads as a manifesto for the reclaiming of data science by academic statistics. In it, he defines six divisions of Greater Data Science, each containing a set of subactivities that roughly map to the pipeline model described here.\nIt is important to note that Donoho’s model is more abstract than a pipeline description and therefore not all of the divisions and subactivities directly map onto the sequence. Data visualization and Presentation defines a general practice, although from the description it clearly maps onto two phases, Explore and Communicate. Computing with Data refers to knowledge of programming languages for data analysis and data processing as well as knowledge of how to use cluster and cloud computing resources at scale. It also includes how to develop workflows which organize work. Clearly, this area belongs to no phase in particular but instead characterizes the broader context in which the data science pipeline operates. The identification of workflows, which are the focus of the Science about Data Science division, also suggests that Donoho is working at a higher level of abstraction than the other models, which places it alongside the of the current essay. The following phases are inferred from Donoho’s descriptions.\n\nGathering: This includes both experimental design, modern data gathering techniques, and identification of existing data resources, from signal data to websites. \\(\\rightarrow [Plan, Collect]\\)\nPreparation: Identification of anomalies and artifacts in the data and handling them by reformatting, recoding, grouping, smoothing, subsetting, etc. \\(\\rightarrow [Clean]\\)\nExploration: Application of EDA to sanity-check data and expose unexpected features. Includes data visualization, which Donoho separates out into a separate division and combines with visualization activities involved in interpretation and communication. \\(\\rightarrow [Explore]\\)\nModern Databases: Transform and restructure data as found in source files, such as CSV files and spreadsheets, and databases, into a forms more suitable for analysis. \\(\\rightarrow [Prepare]\\)\nMathematical Representations: Application of mathematical structures for to extract features from special kinds of data, including acoustic, image, sensor, and network data. For example, the application of the Fourier transform to acousting data or the wavelet transform to image data. \\(\\rightarrow [Prepare]\\)\nData Modeling: Appliction of methods from both traditional statistics and contemporary machine learning. \\(\\rightarrow [Model]\\)\nPresentation: The creation of sophisticated graphics, dashboards, and visualizations to present conclusions to stakeholders. \\(\\rightarrow [Communicate]\\)\nScience about Data Science: In the spirit of Tukey’s “science of data analysis,” this is the evaluation of what data scientists actually do and produce. Includes the identificatin and study of commonly occurring analytical and processing workflows. \\(\\rightarrow [Reflect]\\)\n\n\n\n20.1.10 Géron on Machine Learning\nKey: Géron\nYear: 2017\nSource: Géron (2017) URL→\nBias: Data Mining\nGéron’s text is a classic among practicing data scientists interested in using machine learning in a business setting, covering everything from regression to deep learning from a practical, code-centric perspective. Written with “minimal theory,” the book demostrates the entrenched nature of the pipeline model, especially as it has been become a software development pattern hard-coded into both SciKit Learn and TensorFlow. This usage reflects the fact that within machine learning, “pipeline” has taken on a more specific meaning — “a sequence of data processing components” — than we are using here. These components are units software within a system, not the phases of labor associated with the work of the data scientist. Nevertheless, Géron’s text describes a labor pipeline within which the software pipeline is embedded, the steps of an “end-to-end” classification project.\n\nLook at the big picture: Frame the problem by defining the business objective and the specific goals of the model. This may include defining a specific performance measure, such as a loss function. Consider that the model is a a means to an end. \\(\\rightarrow [Understand]\\)\nGet the data: This consists of setting up a compute workspace and downloading the data. This step also includes getting to know the data and preparing a test set. \\(\\rightarrow [Get, Prepare]\\)\nDiscover and visualize the data to gain insights: Go into more depth exploring the data, using EDA methods to investigate correlations and experiment with attribute combinations. \\(\\rightarrow [Explore]\\)\nPrepare the data for Machine Learning algorithms: This step involves transforming and structuring the data in forms suitable for the algorithms that with fit the data to a model. This includes imputing missing data, handling non-numeric data, feature scaling, etc. This step contains its own pipeline. \\(\\rightarrow [Clean, Prepare]\\)\nSelect a model and train it: Apply models deemed appropriate to the data and compare results. Apply evaluation methods such as cross-validation to compare model results. \\(\\rightarrow [Model]\\)\nFine-tune your model: Once the list of candidate models is shortened, fine-tune their parameters by using various seach methods, e.g. grid, randomized, or ensemble. Also includes evaluating the models on test sets. \\(\\rightarrow [Model]\\)\nPresent your solution: : This step includes presenting to stakeholders what was learned, what worked and what did not, what assumptions were made, and what the system’s limitations are. It also includes documenting everything, creating user-friendly presentations with clear visualizations and easy-to-remember statements. Géron refers to this as the “prelaunch phase,” presumably because the component must be approved to go on to the next phase. \\(\\rightarrow [Communicate]\\)\nLaunch, monitor, and maintain your system: This step includes converting your model into a production-ready component that can become a functioning piece of the overall pipeline. This may mean creating a web service. \\(\\rightarrow [Deploy]\\)\n\n\n\n20.1.11 Das on Data Science\nKey: Das\nYear: 2019\nSource: Das (2019) URL→\nBias: Data Mining\nThis essay belongs to a genre of self-publication that attempts to explain concepts in data science to the public. It is typical of platforms like Medium and what used to be called the blogosphere. It is included here to represent the commonplace nature of the pipeline as a rhetorical device for explaining data science. Here, the pipeline is called a “life-cycle,” although the term pipeline is used as well. The cyclical nature of the process is emphasized by including the first step as last step of the process. [Note that this essay was removed from the web by the author; a link to Internet Archive URL is included for completeness.]\n\nBusiness Understanding: Understand the problem you are trying to solve and how data can be used to support a solution or decision. Identify central objectives and variables that need to be predicted. (Here the author implies that methods such as regression and clustering are objectives.) \\(\\rightarrow [Understand]\\)\nData Mining: Gathering the data from various sources. This may invovle extracting data from a legacy database or webscraping. The author correctly notes that this step should not be lumped together with cleaning. \\(\\rightarrow [Collect]\\)\nData Cleaning: This step includes cleaning and preparing the data, also know as “data janitor work.” This step takes most of the data scientist’s time because there are so many reasons that data may need cleaning. Also includes handling missing data.\n\\(\\rightarrow [Clean]\\)\nData Exploration: This is the brainstorming phase of data analysis, where one discovers patterns and biases in the data. Involves using the basic tools of EDA but also creating interactive visualizations to allow drilling down into specific points, e.g. to explore the story behind outliers. Here also one begins to form hypotheses about the data to be developed. \\(\\rightarrow [Explore]\\)\nFeature Engineering: This is the process of using domain knowledge to transform the data into informative features that represent the business problem. This stage directly influences the accuracy of the predictive model constructed in the next stage. Methods include feature selection (i.e. dimensionality reduction) and constructing new features that will aid in the modeling process. \\(\\rightarrow [Prepare]\\)\nPredictive Modeling: The application of machine learning methods to the data. Includes training several models and evaluating their performance, as well as applying statistical methods and tests to ensure that the outcomes from the models make sense and are significant. Based on the questions developed in the business understanding stage, this is where a model is selected. Model selection will depend on the size, type and quality of the data, availability of computational resources, and the type of output required. \\(\\rightarrow [Model]\\)\nData Visualization: This step combines expertise from the fields of communication, psychology, statistics, and art, with an ultimate goal of communicating the insightw from the model in a simple yet effective and visually pleasing way. \\(\\rightarrow [Communicate]\\)\n\n\n\n20.1.12 Dataman on Data Science\nKey: Dataman\nYear: 2020\nSource: Dataman (2020) URL→\nBias: Data Mining\nAnother example of a self-published explainer essay, this one describes the data science “modeling process” and aligns it with six consultative roles. The other defines eight steps to the process. Curiously, althhough this pipeline focuses on the details of training models, it does not include training the model itself as a step.\n\nSet the objectives: This step includes defining the goals of the model as well as its scope and risk factors. These will determine what data to collect, and whether the cost to collect the data can be justified by the impact of the model. \\(\\rightarrow [Understand]\\)\nCommunicate with key stakeholders: This step involves ongoing aligning expected outcomes with key stakeholders. This step is unique among the pipelines by being place so early in the process. We associate it with the \\(Understand\\) phase because it essentially broads the group for whom understanding matters. \\(\\rightarrow [Plan]\\)\nCollect the necessary data for exploratory data analysis (EDA): This step combines the \\(Collect\\), \\(Clean\\), and \\(Explore\\) phases. Involves the iterative “curation” of data need to conduct EDA. \\(\\rightarrow [Collect, Clean, Explore]\\)\nDetermine the functional form of the model: In this step, the specific from of the model is defined, including the definition and characterization of the target variable. This step involves model selection and would in practice be closely associated with the next. \\(\\rightarrow [Prepare, Model]\\)\nSplit the data into training and validation This step is concerned with model validation and avoiding overfitting. Data are divided into training and test datasets. It is assumed that test data were separated out prior to the preceding step. Presumably this step includes fitting the models, but this is not explicit. \\(\\rightarrow [Prepare, Model]\\)\nAssess the model performance: This step includes determining the stability of a model over time (generalizability), focusing on the overall fit of the model, the significance of each predictor, and the relationship between the target variable and each predictor. Includes measures such as lift. Clearly this step follows the process of fitting and tuning models. \\(\\rightarrow [Model]\\)\nDeploy the model for real-time prediction: The deployment of machine learning models into production, e.g. via batch prediction as a webservice. \\(\\rightarrow [Deploy]\\)\nRe-build the model: This step involves revisiting the pipeline as models lose their predictability due to a variety of causes. Effectively, this step asserts the cyclic and interative nature of the pipeline and therefore belongs to no step in particular.\n\n\n\n20.1.13 Porter on Data Science\nKey: Porter\nYear: 2020\nSource: Porter (2020)\nBias: Statistics \nMichael Porter is an Associate Professor of Data Science and Systems Engineering at the UVA. This essay is an internal report (available on request) on the field of data science from the perspective of curricular planning. Porter argues that Data Science includes seven areas, each of which can be viewed as a science, i.e. as requiring specific expertise. Like Donoho’s essay (and the current), the model presented is more abstract than a pipeline model and includes areas that cross-cut steps in the Primary Sequence. Nevertheless, it retains a sequential structure consistent with the general pattern.\n\nData Collection and Acquisition: The science of “how” and “when” data is collected, and includes all methods of data acquisition from its production through designed experiments to its consumption from external sources, e.g. databases and APIs. \\(\\rightarrow [Collect]\\)\nData Storage and Representation: The science of “how” and “when” data is collected, including data modeling and storing data in databases and files of various formats. Also includes transforming data into “tidy” format. \\(\\rightarrow [Store]\\)\nData Manipulation and Transformation: The science of preparing data for analysis, including wrangling, cleaning, and importing data from databases (after they have been stored in the previous step). \\(\\rightarrow [Clean, Prepare]\\)\nComputing with Data: The science of computing for data analysis with a focus on algorithm design and performance evaluation. \\(\\rightarrow [Model]\\)\nData Analytics: The science of machine learning, broadly conceived to include methods ranging from geneative modeling (either frequentist or Bayesian) and inference to predictive modeling and optimization. Notably, this step also includes EDA and feature engineering. \\(\\rightarrow [Explore, Prepare, Model]\\)\nSummarizing and Communicating Data and Models: The science of extracting and summarizing the information in data for human understanding. This area includes visualization in the context of both EDA and presentation of results to external stakeholders. It also includes the communication of model and data properties (such as bias) to guide interpretation of results. Here we map it to the latter, but note that this area includes at least two steps. In addition, we may may the work of summarization to interpretation. \\(\\rightarrow [Interpret, Communicate]\\)\nPracticing Data Science: The science of the overall system of data science, including improving the data science spipeline, replicability of results, openness and transparency, project management, etc. \\(\\rightarrow [Reflect]\\)\nDisciplinary Data Science: The science of applying data science to specific disciplines. This involves a consideration of how the pipeline operates in different contexts, including how domain knowledge informs each of the steps of the pipeline, from mode of data acquisition to model selection and analytic appraoch, to the interpretation and communication of results. Although placed at the end of the list, it properly belongs to the initial \\(Understand\\) step. \\(\\rightarrow [Understand]\\)"
  },
  {
    "objectID": "appendix-sources.html#summary-table",
    "href": "appendix-sources.html#summary-table",
    "title": "20  Primary Sources",
    "section": "20.2 Summary Table",
    "text": "20.2 Summary Table\n\n\n\n\n\n  \n    \n      \n      understand\n      plan\n      collect\n      store\n      clean\n      explore\n      prepare\n      model\n      interpret\n      communicate\n      deploy\n      reflect\n    \n    \n      \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n       \n    \n  \n  \n    \n      Tukey\n      \n      ■\n      ■\n      \n      \n      ■\n      ■\n      ■\n      ■\n      \n      \n      \n    \n    \n      KDD\n      \n      \n      ■\n      \n      ■\n      \n      ■\n      ■\n      ■\n      \n      \n      \n    \n    \n      SEMMA\n      \n      \n      ■\n      \n      \n      ■\n      ■\n      ■\n      ■\n      \n      \n      \n    \n    \n      Hayashi\n      ■\n      ■\n      ■\n      ■\n      \n      ■\n      ■\n      ■\n      ■\n      \n      \n      \n    \n    \n      CRISPDM\n      ■\n      ■\n      ■\n      ■\n      ■\n      ■\n      ■\n      ■\n      ■\n      ■\n      ■\n      \n    \n    \n      OSEMI\n      \n      \n      ■\n      \n      ■\n      ■\n      ■\n      ■\n      ■\n      \n      \n      \n    \n    \n      Ojeda+\n      ■\n      \n      ■\n      ■\n      ■\n      \n      ■\n      ■\n      \n      ■\n      ■\n      \n    \n    \n      Caffo+\n      ■\n      ■\n      ■\n      \n      \n      ■\n      \n      ■\n      ■\n      ■\n      \n      \n    \n    \n      Donoho\n      \n      ■\n      ■\n      \n      ■\n      ■\n      ■\n      ■\n      \n      ■\n      \n      ■\n    \n    \n      Géron\n      ■\n      \n      ■\n      \n      ■\n      ■\n      ■\n      ■\n      \n      ■\n      ■\n      \n    \n    \n      Das\n      ■\n      \n      ■\n      \n      ■\n      ■\n      ■\n      ■\n      \n      ■\n      \n      \n    \n    \n      Dataman\n      ■\n      \n      ■\n      \n      ■\n      ■\n      ■\n      ■\n      \n      \n      ■\n      \n    \n    \n      Porter\n      ■\n      \n      ■\n      ■\n      ■\n      \n      ■\n      ■\n      ■\n      ■\n      \n      ■"
  },
  {
    "objectID": "appendix-sources.html#references",
    "href": "appendix-sources.html#references",
    "title": "20  Primary Sources",
    "section": "20.3 References",
    "text": "20.3 References\n\n\nAzevedo, Ana Isabel Rojão Lourenço, and Manuel Filipe Santos. 2008. “KDD, SEMMA and CRISP-DM: A Parallel Overview.” IADS-DM.\n\n\nBreiman, Leo. 2001. “Statistical Modeling: The Two Cultures.” Statistical Science 16 (3): 199–231. https://doi.org/10.1214/ss/1009213726.\n\n\nCaffo, Brian, Roger D. Peng, and Jeffrey Leek. 2015. Executive Data Science. Leanpub.\n\n\nConway, Drew. 2010. “The Data Science Venn Diagram.” Drew Conway.\n\n\nDas, Sangeet Moy. 2019. “Data Science Life Cycle 101 for Dummies Like Me.” Medium. https://web.archive.org/web/20191113225625/https://towardsdatascience.com/data-science-life-cycle-101-for-dummies-like-me-e66b47ad8d8f?gi=261acdd4c903.\n\n\nDataman, Dr. 2020. “Data Science Modeling Process & Six Consultative Roles.” Medium. https://towardsdatascience.com/data-science-modeling-process-fa6e8e45bf02.\n\n\nDavidian, Marie. 2013. “Aren’t We Data Science?” AMSTAT News: The Membership Magazine of the American Statistical Association, no. 433: 3.\n\n\nDonoho, David. 2017. “50 Years of Data Science.” Journal of Computational and Graphical Statistics 26 (4): 745–66. https://doi.org/10.1080/10618600.2017.1384734.\n\n\nFayyad, Usama M, Gregory Piatetsky-Shapiro, Padhraic Smyth, et al. 1996. “Knowledge Discovery and Data Mining: Towards a Unifying Framework.” In KDD, 96:82–88.\n\n\nGéron, Aurélien. 2017. Hands-On Machine Learning with Scikit-Learn and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems. 1 edition. Beijing ; Boston: O’Reilly Media.\n\n\nHayashi, Chikio, Keiji Yajima, Hans H. Bock, Noboru Ohsumi, Yutaka Tanaka, and Yasumasa Baba, eds. 1998. Data Science, Classification, and Related Methods: Proceedings of the Fifth Conference of the International Federation of Classification Societies (IFCS-96), Kobe, Japan, March 27, 1996. Studies in Classification, Data Analysis, and Knowledge Organization. Springer Japan. https://doi.org/10.1007/978-4-431-65950-1.\n\n\nMason, Hilary, and Christopher Wiggins. 2010. “A Taxonomy of Data Science.” Dataists.\n\n\nOjeda, Tony, Sean Patrick Murphy, Benjamin Bengfort, and Abhijit Dasgupta. 2014. Practical Data Science Cookbook. Packt Publishing Ltd.\n\n\nPorter, Michael D. 2020. “A Framework for Data Science.”\n\n\nRodriguez, Robert. 2012. “Big Data and Better Data.” Amstat News.\n\n\nTukey, John W. 1962. “The Future of Data Analysis.” The Annals of Mathematical Statistics 33 (1): 1–67.\n\n\nWirth, Rüdiger, and Jochen Hipp. 1999. “CRISP-DM: Towards a Standard Process Model for Data Mining.”"
  },
  {
    "objectID": "BSDS-Course-Info.html",
    "href": "BSDS-Course-Info.html",
    "title": "21  BSDS-Course-Info",
    "section": "",
    "text": "DS 2006\nDS 3005 Mathematics for Data Science"
  }
]