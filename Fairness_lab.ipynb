{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LABS-5: Fairness in Machine Learning\n",
    "\n",
    "In this lab we will explore an example of unnmitigated machine learning to observe how a model performs for different protected classes. We will be using the Fairlearn package to calculate various metrics to help us understand how our model performs across different classes. \n",
    "\n",
    "Check out their user guide for more information on the package! https://fairlearn.org/v0.10/user_guide/fairness_in_machine_learning.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may have to run this cell to install the fairlearn package we'll be using today. You should only have to do this once.\n",
    "\n",
    "!pip install fairlearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import fairlearn.metrics\n",
    "from fairlearn.metrics import MetricFrame\n",
    "from fairlearn.metrics import count, true_positive_rate, false_positive_rate, selection_rate, demographic_parity_ratio\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) ## ignore deprecation warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adult Census Data\n",
    "\n",
    "In this model we will be using demographic variables from census data to predict whether someone makes >50k or <=50k using data from: https://archive.ics.uci.edu/dataset/2/adult. A truncated/precleaned version of this is accessible through the `fairlearn` package, so we will import it from there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in and explore/clean up the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import and view the data\n",
    "\n",
    "from fairlearn.datasets import fetch_adult\n",
    "census_raw = fetch_adult(as_frame=True)\n",
    "census = census_raw.frame #this grabs the data in a pd.dataframe format\n",
    "\n",
    "\n",
    "census.head() # this prints the first 5 lines so we can see the format of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create lists of categorical/numerical columns\n",
    "\n",
    "census_catcols = list(census.select_dtypes('category')) # categorical columns\n",
    "\n",
    "census_numcols = list(set(census.columns) - set(census_catcols)) # numerical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get some info on the numerical data - gives us a general idea of spread and center\n",
    "census.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualize the spread of numeric data\n",
    "\n",
    "fig, axs = plt.subplots(3,2)\n",
    "axs = axs.ravel()\n",
    "for idx,ax in enumerate(axs):\n",
    "    ax.hist(census[census_numcols[idx]])\n",
    "    ax.set_title(census_numcols[idx])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## info on the categorical data\n",
    "# This shows us the levels in the categories for the first 2 category columns\n",
    "\n",
    "for col in census_catcols[:2]:\n",
    "    print(census[col].value_counts(), \"\\n\")\n",
    "\n",
    "# Most of the columns have a ton of categories, we can combine some of them to collapse the categories. \n",
    "# Typically we don't want ot have more than 5ish categories in a given column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Collapsing some categories...\n",
    "\n",
    "# combining similar working classes\n",
    "census['workclass'].replace(['Without-pay', 'Never-worked',], 'No-inc', inplace=True)\n",
    "census['workclass'].replace(['Local-gov', 'State-gov', 'Federal-gov'], 'Gov', inplace=True)\n",
    "# print(census['workclass'].value_counts())\n",
    "\n",
    "# making race binary White/Non-White\n",
    "census['race'] = (census.race.apply(lambda x: x if x == 'White' else \"Non-White\")).astype('category')\n",
    "# print(census['race'].value_counts())\n",
    "\n",
    "# combining similar education classes\n",
    "census['education'].replace(['11th', '10th', '9th', '12th',], 'Some-HS', inplace=True)\n",
    "census['education'].replace(['7th-8th', '5th-6th', '1st-4th', 'Preschool',], 'No-HS', inplace=True)\n",
    "census['education'].replace(['Assoc-voc', 'Assoc-acdm', 'Prof-school'], 'Continued Ed', inplace=True)\n",
    "census['education'].replace(['Bachelors', 'Masters', 'Doctorate'], 'College_+', inplace=True)\n",
    "# print(census['education'].value_counts())\n",
    "\n",
    "# combining similar marital statuses\n",
    "census['marital-status'].replace(['Married-civ-spouse', 'Married-spouse-absent', 'Married-AF-spouse'], 'Married', inplace=True)\n",
    "census['marital-status'].replace(['Divorced', 'Separated', 'Widowed'], 'Was-Married', inplace=True)\n",
    "# print(census['marital-status'].value_counts())\n",
    "\n",
    "# keeping only the top 4 countries (based on number of observations), grouping all others into \"Other\" category\n",
    "top_country = census['native-country'].value_counts()[:5]\n",
    "census['native-country'] = (census['native-country'].apply(lambda x: x if x in top_country else \"Other\")).astype('category')\n",
    "# print(census['native-country'].value_counts())\n",
    "\n",
    "# keeping only the top 4 occupations (based on number of observations), grouping all others into \"Other\" category\n",
    "top_occ = census['occupation'].value_counts()[:5]\n",
    "census['occupation'] = (census['occupation'].apply(lambda x: x if x in top_occ else \"Other\")).astype('category')\n",
    "# print(census['occupation'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A little more pre-processing... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale numbers, One hot encode categories\n",
    "\n",
    "census[census_numcols] = MinMaxScaler().fit_transform(census[census_numcols]) #scale the numerical values so they are all on the same scale\n",
    "census_onehot = pd.get_dummies(census, columns = census_catcols) # creates dummy variables to one-hot encode all categorical variables\n",
    "\n",
    "## One hot encoding creates a column for each category in a feature and assigns it a True/False value. \n",
    "## For example, the 'workclass' column will be broken up into a column for each category ('workclass_Gov', 'workclass_No-inc', etc). \n",
    "## A government workclass observation would have a True value in the 'workclass_Gov' column and a False value in all the other workclass columns.\n",
    "## This is a common strategy you'll see in machine learning - also with 1/0 values instead of True/False (respectively)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "census_onehot.drop(['class_<=50K', 'race_White', 'sex_Male'], axis=1, inplace=True) # drop binary category duplicates\n",
    "census_onehot.head() # visualize what the data looks like after being scaled/one hot encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need to split it into a training set (to build our model) and testing set (to see how it performs on data it was not trained on).\n",
    "\n",
    "We also need to split our data into our target (\"class_>50K\" - denoted as y) and features (everything else - denoted as x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into train and test for model\n",
    "\n",
    "#seperate into features and target (\"class_>50K\")\n",
    "census_x = census_onehot.loc[:, census_onehot.columns != \"class_>50K\"]\n",
    "census_y = census_onehot.loc[:, census_onehot.columns == \"class_>50K\"]\n",
    "\n",
    "#train/test split (75/25)\n",
    "X_train, X_test, y_train, y_test = train_test_split(census_x, census_y, test_size=0.25, random_state=9658)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now, let's look at our data and model and evaluate the fairness\n",
    "\n",
    "You will answer the following questions using the code/output below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "1. The metrics we will be using in this lab are True Positive Rate, False Positive Rate, Selection Rate, Demographic Parity Ratio, and Equalized Odds Ratio. Define each of these metrics, including any relevant math equations, the range (i.e. what number represents complete fairness? Unfairness?), and the situational implication of what this metric represents. \n",
    "\n",
    "2. What are the protected classes in this dataset? Are these classes equally represented in the data? \n",
    "\n",
    "3. For each protected class, what group is being favored in the model?\n",
    "\n",
    "4. Based on the fairness metrics you observed, is the model fair â€“ why/why not? \n",
    "\n",
    "5. Given the goals of this model, do you think it should be used? Why/why not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at the data distribution\n",
    "\n",
    "Type the name of the protected class you'd like to explore in the quotes below. Be sure to use the exact name (case sensitive!) of the column from the data frame above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "protectedClass = \"sex\" # type the protected class you'd like to explore in the quotes here\n",
    "\n",
    "print(census[protectedClass].value_counts()) #print the number of observations in each class\n",
    "\n",
    "#visualize the difference in class representation\n",
    "plt.bar(census[protectedClass].value_counts().index.values, census[protectedClass].value_counts().values)\n",
    "plt.ylabel('count')\n",
    "plt.xlabel(protectedClass)\n",
    "plt.title(f\"Proctected Class Distribution - {protectedClass}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model buliding\n",
    "\n",
    "It's finally time to build our model!\n",
    "\n",
    "We'll be building a simple logistic regression model to predict if a person makes more than 50k a year.\n",
    "\n",
    "Basically, a logistic regression works by calculating a *probability* of an observation being in a spcified class for the target variable. So in this case, our model will produce a probability of a person making more than 50k. This probability is compared to a threshold value, and if the probability is above the threshold is will be categorized as a positive outcome (in this case, making more thank 50k). For more information on logistic regressions, check out this IBM page: https://www.ibm.com/topics/logistic-regression#:~:text=Logistic%20regression%20estimates%20the%20probability,given%20dataset%20of%20independent%20variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train model\n",
    "\n",
    "lreg = LogisticRegression() #initialize a logistic regression model\n",
    "lreg.fit(X_train, y_train) #train this model using our training data\n",
    "\n",
    "y_pred = lreg.predict(X_test) # store predicted values for the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average accuracy on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average accuracy on test data:\\t\",round(lreg.score(X_test, y_test)*100,2),\"%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fairness Metrics\n",
    "\n",
    "We are using the Fairlearn package in Python. \n",
    "\n",
    "You will need to understand what the metric used below mean and how they are calculated. You can find information on the functions used in their documentation: https://fairlearn.org/v0.10/api_reference/index.html#module-fairlearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a function dictionary with the metrics we'd like for each class\n",
    "my_metrics = {\n",
    "    'true positive rate' : true_positive_rate,\n",
    "    'false positive rate' : false_positive_rate,\n",
    "    'selection rate' : selection_rate,\n",
    "    'count' : count\n",
    "}\n",
    "# Construct a MetricFrame for race\n",
    "mf_race = MetricFrame(\n",
    "    metrics=my_metrics,\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred,\n",
    "    sensitive_features=X_test[\"race_Non-White\"]\n",
    ")\n",
    "\n",
    "# Construct a MetricFrame for sex\n",
    "mf_sex = MetricFrame(\n",
    "    metrics=my_metrics,\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred,\n",
    "    sensitive_features=X_test[\"sex_Female\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_confmatrix(y_test, y_pred):\n",
    "    '''\n",
    "    creates a confusion matrix with more descriptive formatting\n",
    "    '''\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel() # grab the individual values\n",
    "    \n",
    "    # create a data frame with the values in the correct spots\n",
    "    conf_matrix = pd.DataFrame({'predicted positive': [tp, fp], \n",
    "                                'predicted negative': [fn, tn]},\n",
    "                                index=['actual positive','actual negative'])\n",
    "    \n",
    "    # return the dataframe to be saved/viewed\n",
    "    return conf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overall metrics\n",
    "\n",
    "Here is the confusion matrix for the model overall with *counts* for the true positive, false positive, true negative, and false negative.\n",
    "\n",
    "For more information on confusion matrices, check out the wiki page: https://en.wikipedia.org/wiki/Confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overall confusion matrix\n",
    "print(\"Confusion matrix for all test data:\") \n",
    "create_confmatrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The overall metrics. You'll use these to compare to with the metrics broken down by each protected class below. \n",
    "## Think about how the differing performance would impact that group based on your understanding of each metric.\n",
    "pd.DataFrame(mf_race.overall, columns = [\"overall\"]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can look at fairness metrics for each protected class:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## metrics broken down by race classes. Compare these to the metrics above. \n",
    "mf_race.by_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derived fairness metrics. Be sure you understand the scale and meaning of these.\n",
    "\n",
    "dpr_race = fairlearn.metrics.demographic_parity_ratio(y_test, y_pred, sensitive_features=X_test.filter(regex=\"race.*\"))\n",
    "print(\"Demographic Parity ratio:\\t\", dpr_race)\n",
    "\n",
    "eodds_race = fairlearn.metrics.equalized_odds_ratio(y_test, y_pred, sensitive_features=X_test.filter(regex=\"race.*\"))\n",
    "print(\"Equalized Odds ratio:\\t\\t\", eodds_race)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## metrics broken down by sex classes. Compare these to the metrics above. \n",
    "\n",
    "mf_sex.by_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derived fairness metrics. Be sure you understand the scale and meaning of these.\n",
    "\n",
    "dpr_sex = fairlearn.metrics.demographic_parity_ratio(y_test, y_pred, sensitive_features=X_test.filter(regex=\"sex.*\"))\n",
    "print(\"Demographic Parity ratio:\\t\", dpr_sex)\n",
    "\n",
    "eodds_sex = fairlearn.metrics.equalized_odds_ratio(y_test, y_pred, sensitive_features=X_test.filter(regex=\"sex.*\"))\n",
    "print(\"Equalized Odds ratio:\\t\\t\", eodds_sex)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
