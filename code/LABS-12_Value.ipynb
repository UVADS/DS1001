{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LABS-12: Value\n",
    "\n",
    "In this lab we will explore and observe how a model performs over different classes using our kNN model that we built in the Analytics lab. We will be using the Fairlearn package to calculate various metrics to help us understand how our model performs across different classes. \n",
    "\n",
    "Check out their [user guide](https://fairlearn.org/v0.10/user_guide/fairness_in_machine_learning.html) for more information on the package!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "\n",
    "import pandas as pd #data ingestion & cleaning\n",
    "import numpy as np #numbers\n",
    "\n",
    "# modeling \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Fairness metrics\n",
    "import fairlearn.metrics\n",
    "from fairlearn.metrics import MetricFrame\n",
    "from fairlearn.metrics import count, true_positive_rate, false_positive_rate, selection_rate, demographic_parity_ratio\n",
    "\n",
    "# visualization\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recreate the model\n",
    "First you will rebuild the model that you made in LABS-9: Analytics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in the data\n",
    "The data you are reading in is the **cleaned** data that you created in LABS-9. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read in cleaned data\n",
    "model_data = pd.read_csv(\"imdb_movies_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the FairLearn package, the target variable must be coded as 0/1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recode 'score' as 0 (low) or 1 (high)\n",
    "model_data['score'] = model_data['score'].map({'low': 0, 'high': 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seperate features from target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features: all columns except 'score'\n",
    "features = model_data.drop('score', axis=1)\n",
    "\n",
    "# Target: score column\n",
    "target = model_data['score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create dummy variables for the features dataframe\n",
    "features = pd.get_dummies(features)\n",
    "\n",
    "# preview the new features dataframe\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "features_train, features_test, target_train, target_test = train_test_split(features, target, test_size=0.2, random_state=45)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model: Build, Train, Test, and Evaluate\n",
    "\n",
    "In the cell below, insert the k value you chose in LABS-9: Analtics for `n_neighbors` before running it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "knn = KNeighborsClassifier(n_neighbors=)  # insert the k value you chose in LABS-9 here!\n",
    "\n",
    "# Train the model\n",
    "knn.fit(features_train, target_train)\n",
    "\n",
    "# Test the model\n",
    "target_predicted = knn.predict(features_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(target_test, target_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Print out the data to use below\n",
    "model_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at the Data for each category\n",
    "\n",
    "Type the name of the category you'd like to explore in the quotes below. Be sure to use the exact name (case sensitive!) of the column from the data frame above.\n",
    "\n",
    "> Use this information to answer **question 2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = \"\" # type the category you'd like to explore in the quotes here\n",
    "\n",
    "print(model_data[category].value_counts()) #print the number of observations in each class\n",
    "\n",
    "#visualize the difference in class representation\n",
    "plt.bar(model_data[category].value_counts().index.values, model_data[category].value_counts().values)\n",
    "plt.ylabel('count')\n",
    "plt.xlabel(category)\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(f\"Class Distribution - {category}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fairness Metrics\n",
    "\n",
    "We are using the Fairlearn package in Python. \n",
    "\n",
    "You will need to understand what the metrics used below mean and how they are calculated. You can find information on the functions used in their documentation: https://fairlearn.org/v0.10/api_reference/index.html#module-fairlearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a function dictionary with the metrics we'd like for each category\n",
    "my_metrics = {\n",
    "    'true positive rate' : true_positive_rate,\n",
    "    'false positive rate' : false_positive_rate,\n",
    "    'selection rate' : selection_rate,\n",
    "    'count' : count\n",
    "}\n",
    "# Construct a MetricFrame for country\n",
    "mf_country = MetricFrame(\n",
    "    metrics=my_metrics,\n",
    "    y_true=target_test,\n",
    "    target_predicted=target_predicted,\n",
    "    sensitive_features=features_test[\"country_AU\"]\n",
    ")\n",
    "\n",
    "# Construct a MetricFrame for top_lang\n",
    "mf_lang = MetricFrame(\n",
    "    metrics=my_metrics,\n",
    "    y_true=target_test,\n",
    "    target_predicted=target_predicted,\n",
    "    sensitive_features=features_test[\"top_lang_ English\"]\n",
    ")\n",
    "\n",
    "# Construct a MetricFrame for top_genre\n",
    "mf_genre = MetricFrame(\n",
    "    metrics=my_metrics,\n",
    "    y_true=target_test,\n",
    "    target_predicted=target_predicted,\n",
    "    sensitive_features=features_test[\"top_genre_Drama\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_confmatrix(target_test, target_predicted):\n",
    "    '''\n",
    "    creates a confusion matrix with more descriptive formatting\n",
    "    '''\n",
    "    tn, fp, fn, tp = confusion_matrix(target_test, target_predicted).ravel() # grab the individual values\n",
    "    \n",
    "    # create a data frame with the values in the correct spots\n",
    "    conf_matrix = pd.DataFrame({'predicted positive': [tp, fp], \n",
    "                                'predicted negative': [fn, tn]},\n",
    "                                index=['actual positive','actual negative'])\n",
    "    \n",
    "    # return the dataframe to be saved/viewed\n",
    "    return conf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall metrics\n",
    "\n",
    "Here is the confusion matrix for the model overall with *counts* for the true positive, false positive, true negative, and false negative.\n",
    "\n",
    "For more information on confusion matrices, check out the wiki page: https://en.wikipedia.org/wiki/Confusion_matrix\n",
    "\n",
    "> Use this information to answer **quesiton 4**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overall confusion matrix\n",
    "print(\"Confusion matrix for all test data:\") \n",
    "create_confmatrix(target_test, target_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The overall metrics. You'll use these to compare to with the metrics broken down by each category below. \n",
    "## Think about how the differing performance would impact that group based on your understanding of each metric.\n",
    "pd.DataFrame(mf_country.overall, columns = [\"overall\"]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics by category\n",
    "\n",
    "Here we are comparing the most prevalent class in each category to the rest of the categories.\n",
    "\n",
    "> Use this information to answer **questions 5 & 6**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `country`\n",
    "\n",
    "The most prevalent country value is \"AU\". We will compare the model performance for the most prevalent country to the performance on the rest of the countries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## metrics broken down by country classes. Compare these to the metrics above. \n",
    "mf_country.by_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derived fairness metrics. Be sure you understand the scale and meaning of these.\n",
    "\n",
    "dpr_country = fairlearn.metrics.demographic_parity_ratio(target_test, target_predicted, sensitive_features=features_test[\"country_AU\"])\n",
    "print(\"Demographic Parity ratio:\\t\", dpr_country)\n",
    "\n",
    "eodds_country = fairlearn.metrics.equalized_odds_ratio(target_test, target_predicted, sensitive_features=features_test[\"country_AU\"])\n",
    "print(\"Equalized Odds ratio:\\t\\t\", eodds_country)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `top_lang`\n",
    "\n",
    "\n",
    "The most prevalent language value is \"English\". We will compare the model performance for the most prevalent language to the performance on the rest of the languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## metrics broken down by top_lang classes. Compare these to the metrics above. \n",
    "\n",
    "mf_lang.by_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derived fairness metrics. Be sure you understand the scale and meaning of these.\n",
    "\n",
    "dpr_lang = fairlearn.metrics.demographic_parity_ratio(target_test, target_predicted, sensitive_features=features_test[\"top_lang_ English\"])\n",
    "print(\"Demographic Parity ratio:\\t\", dpr_lang)\n",
    "\n",
    "eodds_lang = fairlearn.metrics.equalized_odds_ratio(target_test, target_predicted, sensitive_features=features_test[\"top_lang_ English\"])\n",
    "print(\"Equalized Odds ratio:\\t\\t\", eodds_lang)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `top_genre`\n",
    "\n",
    "\n",
    "The most prevalent genre value is \"Drama\". We will compare the model performance for the most prevalent genre to the performance on the rest of the genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## metrics broken down by top_genre classes. Compare these to the metrics above. \n",
    "\n",
    "mf_genre.by_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derived fairness metrics. Be sure you understand the scale and meaning of these.\n",
    "\n",
    "dpr_genre = fairlearn.metrics.demographic_parity_ratio(target_test, target_predicted, sensitive_features=features_test[\"top_genre_Drama\"])\n",
    "print(\"Demographic Parity ratio:\\t\", dpr_genre)\n",
    "\n",
    "eodds_genre = fairlearn.metrics.equalized_odds_ratio(target_test, target_predicted, sensitive_features=features_test[\"top_genre_Drama\"])\n",
    "print(\"Equalized Odds ratio:\\t\\t\", eodds_genre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
