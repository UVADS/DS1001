{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DS 1001: Think Like a Data Scientist\n",
    "\n",
    "\n",
    "This notebook takes the place of slides for the lecture on 2025-04-08 entitled \"Probability Examples\". It is a follow on to the probability sequence which started with a lecture on Probability followed by the Mastermind lab.\n",
    "\n",
    "Key concepts before begining:\n",
    "* Rules of probability (formally Kolmogorov axioms)\n",
    "* Fair coin example\n",
    "* Fair die example\n",
    "* Expressing results in terms of bits (aka yes/no questions)\n",
    "* The \"flat\" probability distribution (formally Discrete Uniform Distribution)\n",
    "* Entropy ( $H = \\log_2(N)$ )\n",
    "\n",
    "Our goal with this example is to develop an intuitive understanding of the fair coin example and extend to all values of $N$ and $p$. This will be demonstrated by simulating coin flips with a random number generator. We will conclude by comparing the simulated results to the theoretical form as well as notice the connection between infinite coin flips and the Normal Distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Science ---> Analytics Module\n",
    "![](analytics.png)\n",
    "\n",
    "Our framing for the analytics module is that it is all about pattern recognition. Humans are built to recognize patterns, but not in a string of 1s and 0s that is millions of bits long (and if we're serious, way way longer than that). We will be using the power of our computer systems to apply mathematical pattern recognition algorithms to those 1s and 0s. Of course there is always the challenge of getting those patterns back out as useful information, but that is the purview of what we call Design."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability Refresher\n",
    "We introduced probability last week in lecture. Then had a lab that used and expanded the idea into the space of information (by calculating Entropy ~ $H = \\log_2(N)$ ).\n",
    "\n",
    "### Rules of Probability\n",
    "We established a few \"rules\" of probability (formally known as Kolmogorov axioms) and how to calculate probabilites (just count the numerator and demonenator).\n",
    "\n",
    "$P(E) \\geq 0$; (also $P(E) \\leq 1$ not technically part of the axioms but it is true as a result)\n",
    "\n",
    "$P(E) = \\frac{\\text{\\# you want}}{P(\\Omega)}$\n",
    "\n",
    "$P(\\Omega) = 1$ (one of the possibilities will occur)\n",
    "\n",
    "#### Examples\n",
    "\n",
    "* Fair coin flip: $P(heads)=\\frac{\\text{head}}{\\text{head}\\ +\\ \\text{tail}}=\\frac{1}{2}$\n",
    "\n",
    "### Measuring Information\n",
    "We use the concept of Entropy ($H$) to quantify Fisher Information in bits.\n",
    "\n",
    "1 fair coin flip = heads or tails = 1 yes/no question = 1 bit\n",
    "\n",
    "$H = \\log_2(N)$\n",
    "\n",
    "#### Examples\n",
    "* one coin flip has $N=2$ so $H=\\log_2(2)=1$ bit\n",
    "* three coin flips has $N=2^3=8$ so $H=\\log_2(2)=3$ bits\n",
    "* Wordle has $N=12,972$ so $H=\\log_2(12,972)=13.7$ bits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Have you read the rubric for the ESSY assignment?\n",
    "### Promoted Cell\n",
    "\n",
    "You should check out the ESSY rubric. The assignment serves as the final assignment for the class and is due on April 29. In the rubric you will notice it consists of 3 parts and that each part requires a figure (you remember how to do figures from Lab 1, right?)\n",
    "\n",
    "Link to rubric - [link](https://myuva.sharepoint.com/:w:/s/DS1001/Ecxqj0AISdRGmK9572bp2jkBrZUon5iujtg70oOmkVIrxQ?e=JfCsVh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability Distributions\n",
    "Up until this point we have used only one probability distribution, the **Discrete Uniform Distribution**. In fact we haven't directly addressed them at all. We have used it via code words like \"fair\" when talking about fliping coins. (We have also assumed independence but that is another lecture entirely).\n",
    "\n",
    "Now we will address the distribution head on because we are going to start generating data with a simulation and are required to name the distribution. From this uniform distribution we will build a more complicated distribution called the **Binomial Distribution**. The binomal distribution expresses the results of a series of independent yes/no questions. (N.B. A coin flip carries the same information in bits as a yes/no question and is generated from the uniform distribution so it is no surprise that a series of coin flips would yield the binomial).\n",
    "\n",
    "Finally we will take the binomal distribution to the extreme and make a connection to our old friend the **Normal Distribution** (ok, technically we have been talking about one probability distribution all course long)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reminder: Guest Speaker on Tuesday 4/15\n",
    "### Promoted Cell\n",
    "\n",
    "Our guest speaker next week will be Miriam Friedel, VP Machine Learning at Capital One (What's in your walled?)\n",
    "She will be talking about her perspective on Data Science in the real world in the context of one of the largest banks in the world. She will also talk about her path from college student to VP. There will be ample time for questions. And if you haven't been to class in a while, it's no problem, we will be happy to see you for this excellent guest lecture."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
