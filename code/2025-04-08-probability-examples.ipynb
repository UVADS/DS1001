{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DS 1001: Think Like a Data Scientist\n",
    "\n",
    "\n",
    "This notebook takes the place of slides for the lecture on 2025-04-08 entitled \"Probability Examples\". It is a follow on to the probability sequence which started with a lecture on Probability followed by the Mastermind lab.\n",
    "\n",
    "Key concepts before begining:\n",
    "* Rules of probability (formally Kolmogorov axioms)\n",
    "* Fair coin example\n",
    "* Fair die example\n",
    "* Expressing results in terms of bits (aka yes/no questions)\n",
    "* The \"flat\" probability distribution (formally Discrete Uniform Distribution)\n",
    "* Entropy ( H = log2(N) )\n",
    "\n",
    "Our goal with this example is to develop an intuitive understanding of the fair coin example and extend to all values of $N$ and $p$. This will be demonstrated by simulating coin flips with a random number generator. We will conclude by comparing the simulated results to the theoretical form as well as notice the connection between infinite coin flips and the Normal Distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Science ---> Analytics Module\n",
    "![](analytics.png)\n",
    "\n",
    "Our framing for the analytics module is that it is all about pattern recognition. Humans are built to recognize patterns, but not in a string of 1s and 0s that is millions of bits long (and if we're serious, way way longer than that). We will be using the power of our computer systems to apply mathematical pattern recognition algorithms to those 1s and 0s. Of course there is always the challenge of getting those patterns back out as useful information, but that is the purview of what we call Design."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability Refresher\n",
    "We introduced probability last week in lecture. Then had a lab that used and expanded the idea into the space of information (by calculating Entropy ~ H = log2(N) ).\n",
    "\n",
    "We established a few \"rules\" of probability (formally known as Kolmogorov axioms) and how to calculate probabilites (just count the numerator and demonenator).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Have you read the rubric for the ESSY assignment?\n",
    "### Promoted Cell\n",
    "\n",
    "You should check out the ESSY rubric. The assignment serves as the final assignment for the class and is due on April 29. In the rubric you will notice it consists of 3 parts and that each part requires a figure (you remember how to do figures from Lab 1, right?)\n",
    "\n",
    "Link to rubric - [link](https://myuva.sharepoint.com/:w:/s/DS1001/Ecxqj0AISdRGmK9572bp2jkBrZUon5iujtg70oOmkVIrxQ?e=JfCsVh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reminder: Guest Speaker on Tuesday 4/15\n",
    "### Promoted Cell\n",
    "\n",
    "Our guest speaker next week will be Miriam Friedel, VP Machine Learning at Capital One (What's in your walled?)\n",
    "She will be talking about her perspective on Data Science in the real world in the context of one of the largest banks in the world. She will also talk about her path from college student to VP. There will be ample time for questions. And if you haven't been to class in a while, it's no problem, we will be happy to see you for this excellent guest lecture."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
